{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport ctypes\nimport os\nimport itertools\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport pprint\nimport time\nimport copy\nimport lightgbm as lgb\nimport torch\nimport polars as pl\nimport optuna\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression,Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\n%matplotlib inline\nfrom random import choice, choices\nfrom functools import reduce, partial\nfrom tqdm import tqdm\nfrom itertools import cycle\nfrom collections import Counter\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nfrom transformers import BertTokenizer\nfrom collections import Counter, defaultdict\nfrom tqdm.autonotebook import tqdm\nfrom math import sqrt\nfrom sklearn import model_selection\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T04:24:43.576557Z","iopub.execute_input":"2024-01-01T04:24:43.576904Z","iopub.status.idle":"2024-01-01T04:24:56.014173Z","shell.execute_reply.started":"2024-01-01T04:24:43.576875Z","shell.execute_reply":"2024-01-01T04:24:56.012948Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def getEssays(df):\n  \n    # 'id', 'activity', 'cursor_position', 'text_change' 열만 선택한 DataFrame 복사\n    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    \n    # 'activity' 열에서 'Nonproduction'인 행을 제외\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n\n    # 각 'id'별로 발생한 활동 수를 계산하여 배열로 저장\n    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n\n    lastIndex = 0\n\n    # 결과를 저장할 Pandas Series 생성\n    essaySeries = pd.Series()\n\n    for index, valCount in enumerate(valCountsArr):\n\n        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n        lastIndex += valCount\n        essayText = \"\"\n\n        for Input in currTextInput.values:\n            \n            # Input[0] = activity\n            # Input[2] = cursor_position\n            # Input[3] = text_change\n            \n            if Input[0] == 'Replace':\n                # '=>' 문자열을 기준으로 text_change를 분할\n                replaceTxt = Input[2].split(' => ')\n                \n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n\n                \n            # If activity = Paste    \n            if Input[0] == 'Paste':\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n\n                \n            # If activity = Remove/Cut\n            if Input[0] == 'Remove/Cut':\n                # DONT TOUCH\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n\n                \n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # \"Move from to\" 텍스트를 제거\n                croppedTxt = Input[0][10:]\n                \n                # ' To '를 기준으로 문자열을 분할\n                splitTxt = croppedTxt.split(' To ')\n                \n                # 문자열을 다시 ', '를 기준으로 분할하여 배열로 저장\n                valueArr = [item.split(', ') for item in splitTxt]\n                \n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n\n                # 같은 위치로 이동하는 경우 건너뛰기\n                if moveData[0] != moveData[2]:\n                    # 텍스트를 앞으로 이동시키는 경우 (다른 경우)\n                    if moveData[0] < moveData[2]:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    else:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue\n                         \n            # If just input\n            # DONT TOUCH\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n\n        # 결과 시리즈의 해당 인덱스에 에세이 텍스트를 설정  \n        essaySeries[index] = essayText\n     \n    # 결과 시리즈의 인덱스를 고유한 'id' 값으로 설정\n    essaySeries.index =  textInputDf['id'].unique()\n    \n    return essaySeries","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:24:56.017088Z","iopub.execute_input":"2024-01-01T04:24:56.018199Z","iopub.status.idle":"2024-01-01T04:24:56.033326Z","shell.execute_reply.started":"2024-01-01T04:24:56.018144Z","shell.execute_reply":"2024-01-01T04:24:56.031689Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\ntrain_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\ntestdf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:24:56.035203Z","iopub.execute_input":"2024-01-01T04:24:56.036536Z","iopub.status.idle":"2024-01-01T04:25:06.486037Z","shell.execute_reply.started":"2024-01-01T04:24:56.036472Z","shell.execute_reply":"2024-01-01T04:25:06.484733Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_essays = getEssays(traindf)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:25:06.487321Z","iopub.execute_input":"2024-01-01T04:25:06.487836Z","iopub.status.idle":"2024-01-01T04:33:55.026762Z","shell.execute_reply.started":"2024-01-01T04:25:06.487801Z","shell.execute_reply":"2024-01-01T04:33:55.025791Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 7min 11s, sys: 1min 37s, total: 8min 48s\nWall time: 8min 48s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntest_essays = getEssays(testdf)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:55.029452Z","iopub.execute_input":"2024-01-01T04:33:55.029814Z","iopub.status.idle":"2024-01-01T04:33:55.043989Z","shell.execute_reply.started":"2024-01-01T04:33:55.029785Z","shell.execute_reply":"2024-01-01T04:33:55.042645Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU times: user 8.31 ms, sys: 1.07 ms, total: 9.38 ms\nWall time: 8.49 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"train_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\ntest_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})\n\nmerged_data = train_essaysdf.merge(train_scores, on='id')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:55.045176Z","iopub.execute_input":"2024-01-01T04:33:55.045747Z","iopub.status.idle":"2024-01-01T04:33:55.065441Z","shell.execute_reply.started":"2024-01-01T04:33:55.045718Z","shell.execute_reply":"2024-01-01T04:33:55.063321Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#단어와 단어의 연속된 조합(단어 n-그램)을 추출하고, 이를 기반으로 텍스트 데이터를 벡터 형태로 변환\n#X_tokenizer_train과 X_tokenizer_test에는 학습 및 테스트 데이터에 대한 단어 빈도 벡터가 저장\n#y는 target value score\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\nX_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\nX_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\ncount_vectorizer.get_feature_names_out() #ADDED\ny = merged_data['score']","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:55.067318Z","iopub.execute_input":"2024-01-01T04:33:55.067779Z","iopub.status.idle":"2024-01-01T04:33:55.977773Z","shell.execute_reply.started":"2024-01-01T04:33:55.067749Z","shell.execute_reply":"2024-01-01T04:33:55.975796Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame()\ndf_test = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:55.979331Z","iopub.execute_input":"2024-01-01T04:33:55.979704Z","iopub.status.idle":"2024-01-01T04:33:55.987172Z","shell.execute_reply.started":"2024-01-01T04:33:55.979675Z","shell.execute_reply":"2024-01-01T04:33:55.985444Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 메모리 요구사항 증가 but, 연산 수행시 효과적: dense 행렬 변환\nX_tokenizer_train = X_tokenizer_train.todense()\nX_tokenizer_test = X_tokenizer_test.todense()\n\nfor i in range(X_tokenizer_train.shape[1]) : \n    L = list(X_tokenizer_train[:,i])\n    li = [int(x) for x in L ]\n    df_train[f'feature {i}'] = li\n    \nfor i in range(X_tokenizer_test.shape[1]) : \n    L = list(X_tokenizer_test[:,i])\n    li = [int(x) for x in L ]\n    df_test[f'feature {i}'] = li   ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:55.988773Z","iopub.execute_input":"2024-01-01T04:33:55.989122Z","iopub.status.idle":"2024-01-01T04:33:59.235679Z","shell.execute_reply.started":"2024-01-01T04:33:55.989092Z","shell.execute_reply":"2024-01-01T04:33:59.233949Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train_index = train_essaysdf['id']\ndf_test_index = test_essaysdf['id']\n\n# id 열 추가 \ndf_train.loc[:, 'id'] = df_train_index\ndf_test.loc[:, 'id'] = df_test_index\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:59.237779Z","iopub.execute_input":"2024-01-01T04:33:59.238100Z","iopub.status.idle":"2024-01-01T04:33:59.245066Z","shell.execute_reply.started":"2024-01-01T04:33:59.238074Z","shell.execute_reply":"2024-01-01T04:33:59.244250Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# 집계함수 적용 df\ntrain_agg_fe_df = traindf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)\n\ntest_agg_fe_df = testdf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:33:59.246167Z","iopub.execute_input":"2024-01-01T04:33:59.247553Z","iopub.status.idle":"2024-01-01T04:34:04.063123Z","shell.execute_reply.started":"2024-01-01T04:33:59.247461Z","shell.execute_reply":"2024-01-01T04:34:04.061193Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nclass Preprocessor:\n    \n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n        \n        self.idf = defaultdict(float)\n#         self.gaps = [1, 2]\n    \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n\n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df):\n        \n        print(\"Starting to engineer features\")\n        \n        # initialize features dataframe\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        # get shifted features\n        # time shift\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # cursor position shift\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # word count shift\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # get aggregate statistical features\n        print(\"Engineering statistical summaries for features\")\n        # [(feature name, [ stat summaries to add ])]\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew'])\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                    \n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        # counts\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        # input words\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n        \n#         # 새로운 정규화된 델타 피처 추가\n#         print(\"< 정규화된 델타 피처 >\")\n#         # 각 이벤트의 시간 범위 계산\n#         temp_delta = df.copy()\n#         temp_delta['delta_e'] = temp_delta['down_time'].shift(-1) - temp_delta['up_time']\n\n#         # 복사 작업의 평균 시간 범위 계산\n#         delta_ecopy = temp_delta['delta_e'].mean()\n\n#         # 정규화 수행\n#         temp_delta['normalized_delta'] = temp_delta['delta_e'] / delta_ecopy\n\n#         # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n#         temp_delta_agg = temp_delta.groupby(\"id\")['normalized_delta'].agg(['mean', 'std']).reset_index()\n#         temp_delta_agg.columns = ['id', 'normalized_delta_mean', 'normalized_delta_std']\n#         feats = feats.merge(temp_delta_agg, on='id', how='left')\n\n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n        \n        \n        # Idle time features(normalized)\n        print(\"< Idle time features(normalized) >\")\n        df['up_time_lagged'] = df.groupby('id')['up_time'].shift()\n        df['time_diff'] = df['down_time'] - df['up_time_lagged']\n        df['time_diff'].fillna(0, inplace=True)\n        time_diff_mean = df['time_diff'].mean()\n        df['time_diff_normalized'] = df['time_diff'] / time_diff_mean\n        idle_time_feats = df[df['activity'].isin(['Input', 'Remove/Cut'])].groupby('id')['time_diff_normalized'].agg(\n            inter_key_largest_latency = 'max',\n            inter_key_median_latency = 'median',\n            mean_pause_time = 'mean',\n            std_pause_time = 'std',\n            total_pause_time = 'sum',\n            pauses_half_sec = lambda x: (x > 0.5).sum(),\n            pauses_1_sec = lambda x: (x > 1).sum()\n        )\n        feats = feats.join(idle_time_feats, on='id', how='left')\n\n            # P-bursts features(normalized)\n        print(\"< P-bursts features(normalized) >\")\n        df['up_time_lagged'] = df.groupby('id')['up_time'].shift()\n        df['time_diff'] = (df['down_time'] - df['up_time_lagged']) / 1000\n        df['is_p_burst'] = (df['activity'].isin(['Input', 'Remove/Cut'])) & (df['time_diff'] < 2)\n        df['p_burst_group'] = df.groupby('id')['is_p_burst'].transform(lambda x: x.ne(x.shift()).cumsum())\n        df['p_burst_count'] = df.groupby(['id', 'p_burst_group'])['is_p_burst'].transform('sum') * df['is_p_burst']\n        p_bursts_feats = df.groupby('id')['p_burst_count'].agg(\n            p_bursts_mean = 'mean',\n            p_bursts_std = 'std',\n            p_bursts_max = 'max',\n            p_bursts_total = 'sum'\n        )\n        feats = feats.join(p_bursts_feats, on='id', how='left')\n\n        # R-bursts features(normalized)\n        print(\"< R-bursts features(normalized) >\")\n        df['is_r_burst'] = df['activity'] == 'Remove/Cut'\n        df['r_burst_group'] = df.groupby('id')['is_r_burst'].transform(lambda x: x.ne(x.shift()).cumsum())\n        df['r_burst_count'] = df.groupby(['id', 'r_burst_group'])['is_r_burst'].transform('sum') * df['is_r_burst']\n        r_bursts_feats = df.groupby('id')['r_burst_count'].agg(\n            r_bursts_mean = 'mean',\n            r_bursts_std = 'std',\n            r_bursts_max = 'max',\n            r_bursts_total = 'sum'\n        )\n        feats = feats.join(r_bursts_feats, on='id', how='left')\n\n        # Normalized Delta Features\n        print(\"< Normalized Delta Features >\")\n        df['delta_e'] = df.groupby('id')['up_time'].shift(-1) - df['down_time']\n        delta_ecopy = df['delta_e'].mean()\n        df['normalized_delta'] = df['delta_e'] / delta_ecopy\n        delta_feats = df.groupby('id')['normalized_delta'].agg(\n            normalized_delta_mean = 'mean',\n            normalized_delta_std = 'std',\n            normalized_delta_max = 'max'\n        )\n        feats = feats.join(delta_feats, on='id', how='left')\n\n        \n        print(\"Done!\")\n        return feats","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:10:52.165005Z","iopub.execute_input":"2024-01-01T09:10:52.165452Z","iopub.status.idle":"2024-01-01T09:10:52.223579Z","shell.execute_reply.started":"2024-01-01T09:10:52.165422Z","shell.execute_reply":"2024-01-01T09:10:52.221615Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# 위 클래스 이용, train,test data -> feature engineering.\npreprocessor = Preprocessor(seed=42)\n\nprint(\"Engineering features for training data\")\n\nother_train_feats = preprocessor.make_feats(traindf)\n\nprint()\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\nprint(\"-\"*25)\nother_test_feats = preprocessor.make_feats(testdf)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:10:54.667351Z","iopub.execute_input":"2024-01-01T09:10:54.669138Z","iopub.status.idle":"2024-01-01T09:15:08.958251Z","shell.execute_reply.started":"2024-01-01T09:10:54.669073Z","shell.execute_reply":"2024-01-01T09:15:08.956199Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Engineering features for training data\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ce47935427453389a43a352387acf6"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866a100e335a47e98fbcfc2d1a0a3ec1"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ee3bc1489ec41be96253bf62f38dadd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4f36c6832a435faef8bd2bdffdbf6f"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9264ca6814494afa9781d806b43955f2"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603e5139c42149dc9774b9cd25f50ae4"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\n< Idle time features(normalized) >\n< P-bursts features(normalized) >\n< R-bursts features(normalized) >\n< Normalized Delta Features >\nDone!\n\n-------------------------\nEngineering features for test data\n-------------------------\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe177c44619a4940b5e5e4cb80d70db0"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b789d4ae3264f6bb1e73464b5717050"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e001cdf0eb54ef2918f22e7d715b31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd1ef7f922042068063d95da213c489"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fee160fad94c119df75ea38508ce36"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36da56b9cee84cc5ab723ddfa4a26f23"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\n< Idle time features(normalized) >\n< P-bursts features(normalized) >\n< R-bursts features(normalized) >\n< Normalized Delta Features >\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train_all = pd.DataFrame()\ndf_test_all = pd.DataFrame()\n\ndf_train_all = df_train.merge(train_agg_fe_df,on='id')\ndf_test_all = df_test.merge(test_agg_fe_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:15:56.488364Z","iopub.execute_input":"2024-01-01T09:15:56.488853Z","iopub.status.idle":"2024-01-01T09:15:56.582590Z","shell.execute_reply.started":"2024-01-01T09:15:56.488823Z","shell.execute_reply":"2024-01-01T09:15:56.579833Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# 1,3 사분위 수 계산\ndef q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:15:59.686252Z","iopub.execute_input":"2024-01-01T09:15:59.686734Z","iopub.status.idle":"2024-01-01T09:15:59.696541Z","shell.execute_reply.started":"2024-01-01T09:15:59.686700Z","shell.execute_reply":"2024-01-01T09:15:59.694037Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', 'sum']\n\ndef split_essays_into_sentences(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n    essay_df = essay_df.explode('sent')\n    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n    return essay_df\n\ndef compute_sentence_aggregations(df):\n    sent_agg_df = pd.concat(\n        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    )\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\ndef split_essays_into_paragraphs(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n    essay_df = essay_df.explode('paragraph')\n    # Number of characters in paragraphs\n    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n    return essay_df\n\ndef compute_paragraph_aggregations(df):\n    paragraph_agg_df = pd.concat(\n        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    ) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:02.894020Z","iopub.execute_input":"2024-01-01T09:16:02.895612Z","iopub.status.idle":"2024-01-01T09:16:02.915900Z","shell.execute_reply.started":"2024-01-01T09:16:02.895551Z","shell.execute_reply":"2024-01-01T09:16:02.913784Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train_sent_df = split_essays_into_sentences(train_essaysdf)\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n\ntrain_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n\ntest_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))\n\ntrain_paragraph_agg_df.loc[:, 'id'] = df_train_index\ntrain_sent_agg_df.loc[:, 'id'] = df_train_index\n\ntest_paragraph_agg_df.loc[:, 'id'] = df_test_index\ntest_sent_agg_df.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:07.382192Z","iopub.execute_input":"2024-01-01T09:16:07.382773Z","iopub.status.idle":"2024-01-01T09:16:19.892814Z","shell.execute_reply.started":"2024-01-01T09:16:07.382733Z","shell.execute_reply":"2024-01-01T09:16:19.890875Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"new_train_feats = pd.DataFrame()\nnew_test_feats = pd.DataFrame()\n\nnew_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\nnew_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')\n\nnew_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\nnew_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')\n\ntrain_feats = pd.DataFrame()\ntest_feats = pd.DataFrame()\n\ntrain_feats = new_train_feats.merge(other_train_feats,on='id')\ntest_feats = new_test_feats.merge(other_test_feats,on='id')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:19.895308Z","iopub.execute_input":"2024-01-01T09:16:19.895694Z","iopub.status.idle":"2024-01-01T09:16:19.998112Z","shell.execute_reply.started":"2024-01-01T09:16:19.895665Z","shell.execute_reply":"2024-01-01T09:16:19.995968Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"data = []\n\nfor logs in [traindf, testdf]:\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n\n    group = logs.groupby('id')['time_diff']\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n\n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:20.000376Z","iopub.execute_input":"2024-01-01T09:16:20.000759Z","iopub.status.idle":"2024-01-01T09:16:28.923777Z","shell.execute_reply.started":"2024-01-01T09:16:20.000730Z","shell.execute_reply":"2024-01-01T09:16:28.921691Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_feats['score_class'] = le.fit_transform(train_feats['score'])","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:28.926478Z","iopub.execute_input":"2024-01-01T09:16:28.926855Z","iopub.status.idle":"2024-01-01T09:16:28.936360Z","shell.execute_reply.started":"2024-01-01T09:16:28.926827Z","shell.execute_reply":"2024-01-01T09:16:28.934800Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"target_col = ['score']\n\ndrop_cols = ['id', 'score_class']\ntrain_cols = list()\n\ntrain_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:28.937968Z","iopub.execute_input":"2024-01-01T09:16:28.938801Z","iopub.status.idle":"2024-01-01T09:16:28.952550Z","shell.execute_reply.started":"2024-01-01T09:16:28.938724Z","shell.execute_reply":"2024-01-01T09:16:28.950419Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n\n# 결측치 최빈값으로 처리 \nfor col in nan_cols:\n    mode_value_train = train_feats[col].mode()[0] #최빈값 여러 개 일 경우 첫 번째꺼 선택 \n    train_feats[col].fillna(mode_value_train, inplace=True)\n    \nfor col in test_feats.columns[test_feats.isna().any()].tolist():\n    # Find the most frequent value in the training set for the current feature\n    most_frequent_value_train = train_feats[col].mode()[0]\n    \n    # Fill missing values in the test set with the most frequent value from the training set\n    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n\ntrain_feats.shape, test_feats.shape   ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:28.954897Z","iopub.execute_input":"2024-01-01T09:16:28.955438Z","iopub.status.idle":"2024-01-01T09:16:29.132710Z","shell.execute_reply.started":"2024-01-01T09:16:28.955402Z","shell.execute_reply":"2024-01-01T09:16:29.130941Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"((2471, 677), (3, 675))"},"metadata":{}}]},{"cell_type":"code","source":"# train_feats.columns\n\n# for col in train_feats.columns:\n#     print(col)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T08:22:17.187982Z","iopub.execute_input":"2024-01-01T08:22:17.188625Z","iopub.status.idle":"2024-01-01T08:22:17.199589Z","shell.execute_reply.started":"2024-01-01T08:22:17.188588Z","shell.execute_reply":"2024-01-01T08:22:17.198025Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# input_word_length_median\n#\n\n#train_feats              = train_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\n#train_feats              = train_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:37:54.857724Z","iopub.execute_input":"2024-01-01T04:37:54.858125Z","iopub.status.idle":"2024-01-01T04:37:54.868785Z","shell.execute_reply.started":"2024-01-01T04:37:54.858088Z","shell.execute_reply":"2024-01-01T04:37:54.867443Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"set(train_feats.columns) - set(test_feats.columns)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:37:54.870978Z","iopub.execute_input":"2024-01-01T04:37:54.872417Z","iopub.status.idle":"2024-01-01T04:37:54.886571Z","shell.execute_reply.started":"2024-01-01T04:37:54.872367Z","shell.execute_reply":"2024-01-01T04:37:54.885605Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'score', 'score_class'}"},"metadata":{}}]},{"cell_type":"code","source":"clean_memory() # 학습 전 garbaage collecting + cache 비우기","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:31.096747Z","iopub.execute_input":"2024-01-01T09:16:31.097101Z","iopub.status.idle":"2024-01-01T09:16:31.582977Z","shell.execute_reply.started":"2024-01-01T09:16:31.097078Z","shell.execute_reply":"2024-01-01T09:16:31.579674Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"models_dict = {}\nscores = []\n\ntest_predict_list = []\nbest_params = {'boosting_type': 'gbdt', \n               'metric': 'rmse',\n               'reg_alpha': 0.003188447814669599, \n               'reg_lambda': 0.0010228604507564066, \n               'colsample_bytree': 0.5420247656839267, \n               'subsample': 0.9778252382803456, \n               'feature_fraction': 0.8,\n               'bagging_freq': 1,\n               'bagging_fraction': 0.75,\n               'learning_rate': 0.01716485155812008, \n               'num_leaves': 19, \n               'min_child_samples': 46,\n               'verbosity': -1,\n               'random_state': 42,\n               'n_estimators': 500,\n               'device_type': 'cpu'}\n\nfor i in range(5): \n    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n\n    oof_valid_preds = np.zeros(train_feats.shape[0], )\n\n    X_test = test_feats[train_cols]\n\n\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n\n        print(\"==-\"* 50)\n        print(\"Fold : \", fold)\n\n        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n        print(\"Trian :\", X_train.shape, y_train.shape)\n        print(\"Valid :\", X_valid.shape, y_valid.shape)\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            'random_state': 42,\n            \"n_estimators\" : 12001,\n            \"verbosity\": -1,\n            \"device_type\": \"cpu\",\n            **best_params\n        }\n\n        model = lgb.LGBMRegressor(**params)\n\n        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n        verbose_callback = lgb.callback.record_evaluation({})\n\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n                  callbacks=[early_stopping_callback, verbose_callback],\n        )\n\n        valid_predict = model.predict(X_valid)\n        oof_valid_preds[valid_idx] = valid_predict\n\n        test_predict = model.predict(X_test)\n        test_predict_list.append(test_predict)\n\n        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n        print(\"Fold RMSE Score : \", score)\n\n        models_dict[f'{fold}_{i}'] = model\n\n\n    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n    scores.append(oof_score)\n    print(\"OOF RMSE Score : \", oof_score)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:16:33.153490Z","iopub.execute_input":"2024-01-01T09:16:33.153913Z","iopub.status.idle":"2024-01-01T09:28:53.178799Z","shell.execute_reply.started":"2024-01-01T09:16:33.153886Z","shell.execute_reply":"2024-01-01T09:28:53.176797Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 674) (2223, 1)\nValid : (248, 674) (248, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5740957517864301\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5152186722180342\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6715255702088492\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6171325475130134\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5861622106033516\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6117744846125793\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6490060159116946\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6368915103986317\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6355460863948602\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5593312163526065\nOOF RMSE Score :  0.6073006311651835\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 674) (2223, 1)\nValid : (248, 674) (248, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5922328424313666\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5827723674122997\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5729467698441957\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5914171488488397\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6036646841441724\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6430467527769574\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6363449326349289\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5768539950267414\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6565473858704785\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6268023718129411\nOOF RMSE Score :  0.6089232888304765\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 674) (2223, 1)\nValid : (248, 674) (248, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6796947617565986\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.589234512216965\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5701040215328359\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5978187241499703\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5747690892209217\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6024651212061768\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6921688325388505\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.565125532190071\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6406659907957671\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5980021024403145\nOOF RMSE Score :  0.612518347357059\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 674) (2223, 1)\nValid : (248, 674) (248, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.620229294510606\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5918343655924867\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6142592952071035\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6033010841998536\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.645232474356129\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5586663016132242\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6276439619278519\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.612555444115543\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6672119483102362\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5539230679239895\nOOF RMSE Score :  0.6104002450907864\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 674) (2223, 1)\nValid : (248, 674) (248, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6042187862194266\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5712806931228672\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6172628432807947\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6058117323339404\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.5822747621035457\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6571178885676028\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6048989050724062\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.615887863533797\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6424223545932083\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 674) (2224, 1)\nValid : (247, 674) (247, 1)\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\nFold RMSE Score :  0.6169050975224527\nOOF RMSE Score :  0.6122720250723261\n","output_type":"stream"}]},{"cell_type":"code","source":"# 모든 폴드에 대한 oof_score 평균  \n# startified 적용하니 살짝 낮아짐. 근데 public lb 증가해서 다시 kfold로 변경 \nnp.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:28:53.181330Z","iopub.execute_input":"2024-01-01T09:28:53.181713Z","iopub.status.idle":"2024-01-01T09:28:53.191154Z","shell.execute_reply.started":"2024-01-01T09:28:53.181685Z","shell.execute_reply":"2024-01-01T09:28:53.189627Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"0.6102829075031663"},"metadata":{}}]},{"cell_type":"code","source":"test_feats['score'] = np.mean(test_predict_list, axis=0)\npubliclgbm_pred = test_feats[['id', 'score']]\npubliclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:28:53.192491Z","iopub.execute_input":"2024-01-01T09:28:53.192847Z","iopub.status.idle":"2024-01-01T09:28:53.208183Z","shell.execute_reply.started":"2024-01-01T09:28:53.192817Z","shell.execute_reply":"2024-01-01T09:28:53.207189Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.532989\n1  2222bbbb  1.468471\n2  4444cccc  1.470221","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.532989</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.468471</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.470221</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\nactivities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\nevents = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\ntext_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n\n\ndef count_by_values(df, colname, values):\n    fts = df.select(pl.col('id').unique(maintain_order=True))\n    for i, value in enumerate(values):\n        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n        fts  = fts.join(tmp_df, on='id', how='left') \n    return fts\n\n\n# time_diff 정규화 피처 추가 \ndef dev_feats(df):\n    \n    print(\"< Count by values features >\")\n    \n    feats = count_by_values(df, 'activity', activities)\n    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n\n    print(\"< Input words stats features >\")\n\n    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n#                              input_word_length_more6 = pl.col('text_change').apply(lambda x: sum(1 for i in x if len(i) >= 6) if len(x) > 0 else 0),\n                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n#                              input_word_length_less5_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x if len(i) < 5] if len(x) > 0 else [0])),\n                             \n                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n#                              input_word_length_4std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0)),\n\n                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n\n                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)),\n#                              input_word_length_4skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0))\n                             )\n\n    temp = temp.drop('text_change')\n    feats = feats.join(temp, on='id', how='left') \n\n    \n    print(\"< Numerical columns features >\")\n\n    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< Categorical columns features >\")\n    \n    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n    feats = feats.join(temp, on='id', how='left') \n    \n    # 유휴 시간 특징 추출\n    print(\"< Idle time features(normalized) >\")\n    \n    \n    # 이전 'up_time' 값을 현재 행으로 이동하고, 'up_time_lagged'라는 새로운 열로 저장\n    df = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    # 'down_time'과 'up_time_lagged'의 차이를 계산하여 'time_diff' 열 생성\n    df = df.with_columns((pl.col('down_time') - pl.col('up_time_lagged')).fill_null(0).alias('time_diff'))\n    # 'time_diff'의 평균 계산 및 정규화\n    time_diff_mean = df.select(pl.mean('time_diff')).collect().get_column('time_diff')[0]\n    df = df.with_columns((pl.col('time_diff') / time_diff_mean).alias('time_diff'))\n    # 필터링 및 통계치 계산\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(\n        inter_key_largest_lantency = pl.max('time_diff'),\n        inter_key_median_lantency = pl.median('time_diff'),\n        mean_pause_time = pl.mean('time_diff'),\n        std_pause_time = pl.std('time_diff'),\n        total_pause_time = pl.sum('time_diff'),\n        pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n        pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n        pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n        pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n        pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),\n    )\n    \n    # 계산된 통계치를 원래의 'feats' 데이터프레임에 병합\n    feats = feats.join(temp, on='id', how='left')\n    \n    # P-버스트 특징 추출\n    print(\"< P-bursts features(normalized) >\")\n    # 위와 같은 방법으로 'up_time'을 이전 행으로 이동\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    # 'down_time'과 'up_time_lagged'의 차이를 초 단위로 계산\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링하고, 시간 차이가 2초 미만인 경우만 선택\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('time_diff') < 2)\n    # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'P-bursts'로 계산\n    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n    # 결측값 제거\n    temp = temp.drop_nulls()\n    # 다양한 'P-bursts' 통계치 계산\n    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n\n    # 계산된 'P-bursts' 통계치를 'feats' 데이터프레임에 병합\n    feats = feats.join(temp, on='id', how='left') \n    # R-버스트 특징 추출\n    print(\"< R-bursts features(normalized) >\")\n    # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    # 'activity' 열이 'Remove/Cut'인 경우만 선택\n    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n    # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'R-bursts'로 계산\n    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n    # 결측값 제거\n    temp = temp.drop_nulls()\n    # 다양한 'R-bursts' 통계치 계산\n    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n\n    # 계산된 'R-bursts' 통계치를 'feats' 데이터프레임에 병합\n    feats = feats.join(temp, on='id', how='left')\n    #######\n    print(\"< Normalized Delta Features >\")\n    # 각 이벤트의 시간 범위 계산\n    temp_delta = df.with_columns((pl.col('down_time').shift(-1) - pl.col('up_time')).alias('delta_e'))\n\n    # 복사 작업의 평균 시간 범위 계산\n    # 'delta_e' 열의 평균을 계산하고 결과 열의 이름을 명시적으로 지정합니다.\n    delta_ecopy = temp_delta.select(pl.mean('delta_e').alias('delta_e_mean')).collect().get_column('delta_e_mean')[0]\n\n    # 정규화 수행\n    temp_delta = temp_delta.with_columns((pl.col('delta_e') / delta_ecopy).alias('normalized_delta'))\n\n    # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n    temp_delta_agg = temp_delta.groupby(\"id\").agg(pl.mean('normalized_delta').suffix('_mean'), \n                                                  pl.std('normalized_delta').suffix('_std'))\n    feats = feats.join(temp_delta_agg, on='id', how='left')\n    \n    \n    print(\"< Idle time features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n                                   inter_key_median_lantency = pl.median('time_diff'),\n                                   mean_pause_time = pl.mean('time_diff'),\n                                   std_pause_time = pl.std('time_diff'),\n                                   total_pause_time = pl.sum('time_diff'),\n                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n    feats = feats.join(temp, on='id', how='left') \n    \n    print(\"< P-bursts features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('time_diff')<2)\n    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< R-bursts features >\")\n\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left')\n    \n    return feats\n\ndef train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x.iloc[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x.iloc[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\ndef evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n        model.fit(train_x, train_y)\n        if test_x is None:\n            test_y[valid_index] = model.predict(valid_x)\n        else:\n            test_y[:, i] = model.predict(test_x)\n    return test_y if (test_x is None) else np.mean(test_y, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T08:55:28.472931Z","iopub.execute_input":"2024-01-01T08:55:28.473400Z","iopub.status.idle":"2024-01-01T08:55:28.525419Z","shell.execute_reply.started":"2024-01-01T08:55:28.473361Z","shell.execute_reply":"2024-01-01T08:55:28.523167Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\nAGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\ndef reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\n\ndef get_essay_df(df):\n    df       = df[df.activity != 'Nonproduction']\n    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n    return essay_df\n\n\ndef word_feats(df):\n    essay_df = df\n    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('word')\n    df['word_len'] = df['word'].apply(lambda x: len(x))\n    df = df[df['word_len'] != 0]\n\n    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n    word_agg_df['id'] = word_agg_df.index\n    word_agg_df = word_agg_df.reset_index(drop=True)\n    return word_agg_df\n\n\ndef sent_feats(df):\n    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('sent')\n    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n    df = df[df.sent_len!=0].reset_index(drop=True)\n\n    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n\ndef parag_feats(df):\n    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n    df = df.explode('paragraph')\n    # Number of characters in paragraphs\n    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n    df = df[df.paragraph_len!=0].reset_index(drop=True)\n    \n    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n\ndef product_to_keys(logs, essays):\n    essays['product_len'] = essays.essay.str.len()\n    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n    essays = essays.merge(tmp_df, on='id', how='left')\n    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n    return essays[['id', 'product_to_keys']]\n\ndef get_keys_pressed_per_second(logs):\n    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n    return temp_df[['id', 'keys_per_second']]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:46:50.967769Z","iopub.execute_input":"2024-01-01T04:46:50.968764Z","iopub.status.idle":"2024-01-01T04:46:50.997110Z","shell.execute_reply.started":"2024-01-01T04:46:50.968728Z","shell.execute_reply":"2024-01-01T04:46:50.995548Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\ntrain_logs    = pl.scan_csv(data_path + 'train_logs.csv')\ntrain_feats   = dev_feats(train_logs)\ntrain_feats   = train_feats.collect().to_pandas()\n\nprint('< Essay Reconstruction >')\ntrain_logs             = train_logs.collect().to_pandas()\ntrain_essays           = get_essay_df(train_logs)\ntrain_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\ntrain_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\ntrain_feats['pw'] = train_feats['paragraph_len_sum']/train_feats['word_len_sum']\ntrain_feats              = train_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\ntrain_feats              = train_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n\nprint('< Mapping >')\ntrain_scores   = pd.read_csv(data_path + 'train_scores.csv')\ndata           = train_feats.merge(train_scores, on='id', how='left')\nx              = data.drop(['id', 'score'], axis=1)\ny              = data['score'].values\nprint(f'Number of features: {len(x.columns)}')\n\n\nprint('< Testing Data >')\ntest_logs   = pl.scan_csv(data_path + 'test_logs.csv')\ntest_feats  = dev_feats(test_logs)\ntest_feats  = test_feats.collect().to_pandas()\n\ntest_logs             = test_logs.collect().to_pandas()\ntest_essays           = get_essay_df(test_logs)\ntest_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\ntest_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\ntest_feats['pw'] = test_feats['paragraph_len_sum']/test_feats['word_len_sum']\ntest_feats              = test_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\ntest_feats              = test_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n\ntest_ids = test_feats['id'].values\ntestin_x = test_feats.drop(['id'], axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:46:51.000764Z","iopub.execute_input":"2024-01-01T04:46:51.001119Z","iopub.status.idle":"2024-01-01T04:48:08.502835Z","shell.execute_reply.started":"2024-01-01T04:46:51.001093Z","shell.execute_reply":"2024-01-01T04:48:08.500799Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features(normalized) >\n< P-bursts features(normalized) >\n< R-bursts features(normalized) >\n< Normalized Delta Features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n< Essay Reconstruction >\n< Mapping >\nNumber of features: 185\n< Testing Data >\n< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features(normalized) >\n< P-bursts features(normalized) >\n< R-bursts features(normalized) >\n< Normalized Delta Features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n","output_type":"stream"}]},{"cell_type":"code","source":"print('< Learning and Evaluation >')\nlgbm_param = {'n_estimators': 1024,\n         'learning_rate': 0.005,\n         'metric': 'rmse',\n         'random_state': 42,\n         'force_col_wise': True,\n         'verbosity': 0,}\nlgbm_solution = LGBMRegressor(**lgbm_param)\nlgbm_pred   = evaluate(x.copy(), y.copy(), lgbm_solution, test_x=testin_x.copy()) \n\nlgbm_sub = pd.DataFrame({'id': test_ids, 'score': lgbm_pred})\n#sub.to_csv('submission.csv', index=False)\nlgbm_sub","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:48:08.504466Z","iopub.execute_input":"2024-01-01T04:48:08.505280Z","iopub.status.idle":"2024-01-01T04:49:45.309116Z","shell.execute_reply.started":"2024-01-01T04:48:08.505245Z","shell.execute_reply":"2024-01-01T04:49:45.307818Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"< Learning and Evaluation >\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.248965\n1  2222bbbb  1.205973\n2  4444cccc  1.254234","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.248965</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.205973</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.254234</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('< XGB Learning and Evaluation >')\n\nxgb_param={\n'reg_alpha': 0.0008774661176012108,\n'reg_lambda': 2.542812743920178,\n'colsample_bynode': 0.7839026197349153,\n'subsample': 0.8994226268096415, \n'eta': 0.04730766698056879, \n'max_depth': 3, \n'n_estimators': 1024,\n'random_state': 42,\n'eval_metric': 'rmse'\n}\n\nxgb_solution = XGBRegressor(**xgb_param)\nxgb_pred   = evaluate(x.copy(), y.copy(), xgb_solution, test_x=testin_x.copy()) \n\nxgb_sub = pd.DataFrame({'id': test_ids, 'score': xgb_pred})\n#xgb_sub.to_csv('submission.csv', index=False)\nxgb_sub","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:49:45.310895Z","iopub.execute_input":"2024-01-01T04:49:45.311347Z","iopub.status.idle":"2024-01-01T04:50:12.334457Z","shell.execute_reply.started":"2024-01-01T04:49:45.311299Z","shell.execute_reply":"2024-01-01T04:50:12.333054Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"< XGB Learning and Evaluation >\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  2.861458\n1  2222bbbb  1.149160\n2  4444cccc  1.309935","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2.861458</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.149160</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.309935</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"best_w =0.7\n\nW = [best_w, 1 - best_w]\nprint(W)\nensemble_preds = lgbm_pred * W[0] + xgb_pred * W[1]\nensemble_preds","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:50:12.335662Z","iopub.execute_input":"2024-01-01T04:50:12.337047Z","iopub.status.idle":"2024-01-01T04:50:12.345870Z","shell.execute_reply.started":"2024-01-01T04:50:12.336993Z","shell.execute_reply":"2024-01-01T04:50:12.344577Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"[0.7, 0.30000000000000004]\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"array([1.73271306, 1.18892935, 1.27094446])"},"metadata":{}}]},{"cell_type":"code","source":"ensemble_sub = pd.DataFrame({'id': test_ids, 'score': ensemble_preds})\nensemble_sub","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:50:12.347586Z","iopub.execute_input":"2024-01-01T04:50:12.347971Z","iopub.status.idle":"2024-01-01T04:50:12.362753Z","shell.execute_reply.started":"2024-01-01T04:50:12.347941Z","shell.execute_reply":"2024-01-01T04:50:12.361146Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.732713\n1  2222bbbb  1.188929\n2  4444cccc  1.270944","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.732713</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.188929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.270944</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def train_valid_split(data_x, data_y, train_idx, valid_idx):\n#     x_train = data_x.iloc[train_idx]\n#     y_train = data_y[train_idx]\n#     x_valid = data_x.iloc[valid_idx]\n#     y_valid = data_y[valid_idx]\n#     return x_train, y_train, x_valid, y_valid\n\n# # LightGBM \n# lgbm_param = {\n#     'n_estimators': 1024,\n#     'learning_rate': 0.005,\n#     'metric': 'rmse',\n#     'random_state': 42\n# }\n\n# # XGB\n# xgb_param = {\n#     'reg_alpha': 0.0008774661176012108,\n#     'reg_lambda': 2.542812743920178,\n#     'colsample_bynode': 0.7839026197349153,\n#     'subsample': 0.8994226268096415,\n#     'eta': 0.04730766698056879, \n#     'max_depth': 3, \n#     'n_estimators': 1024,\n#     'random_state': 42,\n#     'eval_metric': 'rmse'\n# }\n\n# lgbm_model = LGBMRegressor(**lgbm_param)\n# xgb_model = XGBRegressor(**xgb_param)\n\n# # stratified kfold 적용\n# skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n# lgbm_preds = []\n# xgb_preds = []\n# valid_y_list = []\n\n# for train_index, valid_index in skf.split(x, y.astype(str)):     \n#     train_x, train_y, valid_x, valid_y = train_valid_split(x, y, train_index, valid_index)\n\n#     lgbm_model.fit(train_x, train_y)\n#     lgbm_preds.append(lgbm_model.predict(valid_x))\n\n#     xgb_model.fit(train_x, train_y)\n#     xgb_preds.append(xgb_model.predict(valid_x))\n\n#     # 검증 타겟 저장\n#     valid_y_list.append(valid_y)\n\n# # 모든 fold의 예측값과 실제값 통합\n# lgbm_preds = np.concatenate(lgbm_preds)\n# xgb_preds = np.concatenate(xgb_preds)\n# valid_y_combined = np.concatenate(valid_y_list)\n\n# print('lgbm_preds: ', lgbm_preds)\n# print('xgb_preds: ', xgb_preds)\n# print('valid_y_combined: ', valid_y_combined)\n\n\n\n# # 최적 가중치 \n# best_sc = float('inf')\n# best_w = 0\n# for w in np.arange(0, 1.01, 0.01):\n#     combined_pred = w * lgbm_preds + (1-w) * xgb_preds\n#     sc = mean_squared_error(valid_y_combined, combined_pred, squared=False)\n#     if sc < best_sc:\n#         best_sc = sc\n#         best_w = w\n\n# print('최적의 RMSE 점수 = {:.5f}'.format(best_sc))\n# print('최적의 가중치 W = {:.3f}'.format(best_w))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:50:12.366074Z","iopub.execute_input":"2024-01-01T04:50:12.366404Z","iopub.status.idle":"2024-01-01T04:50:12.375428Z","shell.execute_reply.started":"2024-01-01T04:50:12.366378Z","shell.execute_reply":"2024-01-01T04:50:12.373941Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 두 데이터프레임을 'id' 컬럼을 기준으로 병합\nmerged_df = ensemble_sub.merge(publiclgbm_pred, on='id', suffixes=('_ensemble', '_publiclgbm'))\n\n# 가중치 반반\nweight_ensemble = 0.6\nweight_publiclgbm = 0.4\n\n# 가중 평균 점수 계산하여 원래의 'score' 컬럼에 저장\nmerged_df['score'] = (merged_df['score_ensemble'] * weight_ensemble) + (merged_df['score_publiclgbm'] * weight_publiclgbm)\nmerged_df.drop(['score_ensemble', 'score_publiclgbm'], axis=1, inplace=True)\n\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:30:18.636176Z","iopub.execute_input":"2024-01-01T09:30:18.636790Z","iopub.status.idle":"2024-01-01T09:30:18.657368Z","shell.execute_reply.started":"2024-01-01T09:30:18.636734Z","shell.execute_reply":"2024-01-01T09:30:18.655118Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.652824\n1  2222bbbb  1.300746\n2  4444cccc  1.350655","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.652824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.300746</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.350655</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T09:30:23.515909Z","iopub.execute_input":"2024-01-01T09:30:23.516853Z","iopub.status.idle":"2024-01-01T09:30:23.526554Z","shell.execute_reply.started":"2024-01-01T09:30:23.516818Z","shell.execute_reply":"2024-01-01T09:30:23.525156Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
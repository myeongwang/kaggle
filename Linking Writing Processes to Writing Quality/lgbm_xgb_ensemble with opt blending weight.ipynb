{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nimport re\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import skew, kurtosis\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom scipy.stats import skew, kurtosis\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom functools import partial\nfrom sklearn import model_selection,metrics\nimport optuna\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:30:04.462673Z","iopub.execute_input":"2023-12-07T16:30:04.463193Z","iopub.status.idle":"2023-12-07T16:30:09.452892Z","shell.execute_reply.started":"2023-12-07T16:30:04.463127Z","shell.execute_reply":"2023-12-07T16:30:09.451718Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\nactivities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\nevents = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\ntext_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n\n\ndef count_by_values(df, colname, values):\n    fts = df.select(pl.col('id').unique(maintain_order=True))\n    for i, value in enumerate(values):\n        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n        fts  = fts.join(tmp_df, on='id', how='left') \n    return fts\n\n\ndef dev_feats(df):\n    \n    print(\"< Count by values features >\")\n    \n    feats = count_by_values(df, 'activity', activities)\n    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n\n    print(\"< Input words stats features >\")\n\n    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n    temp = temp.drop('text_change')\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Numerical columns features >\")\n\n    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< Categorical columns features >\")\n    \n    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Idle time features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n                                   inter_key_median_lantency = pl.median('time_diff'),\n                                   mean_pause_time = pl.mean('time_diff'),\n                                   std_pause_time = pl.std('time_diff'),\n                                   total_pause_time = pl.sum('time_diff'),\n                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n    feats = feats.join(temp, on='id', how='left') \n    \n    print(\"< P-bursts features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('time_diff')<2)\n    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< R-bursts features >\")\n\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left')\n    \n    return feats\n\n\ndef train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x.iloc[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x.iloc[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\ndef evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n        model.fit(train_x, train_y)\n        if test_x is None:\n            test_y[valid_index] = model.predict(valid_x)\n        else:\n            test_y[:, i] = model.predict(test_x)\n    return test_y if (test_x is None) else np.mean(test_y, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:30:09.454801Z","iopub.execute_input":"2023-12-07T16:30:09.455266Z","iopub.status.idle":"2023-12-07T16:30:09.501749Z","shell.execute_reply.started":"2023-12-07T16:30:09.455234Z","shell.execute_reply":"2023-12-07T16:30:09.500670Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\nAGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\ndef reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\n\ndef get_essay_df(df):\n    df       = df[df.activity != 'Nonproduction']\n    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n    return essay_df\n\n\ndef word_feats(df):\n    essay_df = df\n    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('word')\n    df['word_len'] = df['word'].apply(lambda x: len(x))\n    df = df[df['word_len'] != 0]\n\n    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n    word_agg_df['id'] = word_agg_df.index\n    word_agg_df = word_agg_df.reset_index(drop=True)\n    return word_agg_df\n\n\ndef sent_feats(df):\n    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('sent')\n    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n    df = df[df.sent_len!=0].reset_index(drop=True)\n\n    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n\ndef parag_feats(df):\n    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n    df = df.explode('paragraph')\n    # Number of characters in paragraphs\n    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n    df = df[df.paragraph_len!=0].reset_index(drop=True)\n    \n    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n\ndef product_to_keys(logs, essays):\n    essays['product_len'] = essays.essay.str.len()\n    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n    essays = essays.merge(tmp_df, on='id', how='left')\n    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n    return essays[['id', 'product_to_keys']]\n\ndef get_keys_pressed_per_second(logs):\n    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n    return temp_df[['id', 'keys_per_second']]","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:30:09.503656Z","iopub.execute_input":"2023-12-07T16:30:09.504093Z","iopub.status.idle":"2023-12-07T16:30:09.541930Z","shell.execute_reply.started":"2023-12-07T16:30:09.504061Z","shell.execute_reply":"2023-12-07T16:30:09.540872Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\ntrain_logs    = pl.scan_csv(data_path + 'train_logs.csv')\ntrain_feats   = dev_feats(train_logs)\ntrain_feats   = train_feats.collect().to_pandas()\n\nprint('< Essay Reconstruction >')\ntrain_logs             = train_logs.collect().to_pandas()\ntrain_essays           = get_essay_df(train_logs)\ntrain_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\ntrain_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n\n\nprint('< Mapping >')\ntrain_scores   = pd.read_csv(data_path + 'train_scores.csv')\ndata           = train_feats.merge(train_scores, on='id', how='left')\nx              = data.drop(['id', 'score'], axis=1)\ny              = data['score'].values\nprint(f'Number of features: {len(x.columns)}')\n\n\nprint('< Testing Data >')\ntest_logs   = pl.scan_csv(data_path + 'test_logs.csv')\ntest_feats  = dev_feats(test_logs)\ntest_feats  = test_feats.collect().to_pandas()\n\ntest_logs             = test_logs.collect().to_pandas()\ntest_essays           = get_essay_df(test_logs)\ntest_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\ntest_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n\n\ntest_ids = test_feats['id'].values\ntestin_x = test_feats.drop(['id'], axis=1)\n\nprint('< Learning and Evaluation >')\nparam = {'n_estimators': 1024,\n         'learning_rate': 0.005,\n         'metric': 'rmse',\n         'random_state': 42,\n         'force_col_wise': True,\n         'verbosity': 0,}\nsolution = LGBMRegressor(**param)\ny_pred   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n\nsub = pd.DataFrame({'id': test_ids, 'score': y_pred})\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:30:12.194729Z","iopub.execute_input":"2023-12-07T16:30:12.195144Z","iopub.status.idle":"2023-12-07T16:32:55.658304Z","shell.execute_reply.started":"2023-12-07T16:30:12.195110Z","shell.execute_reply":"2023-12-07T16:32:55.657067Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n< Essay Reconstruction >\n< Mapping >\nNumber of features: 165\n< Testing Data >\n< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n< Learning and Evaluation >\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:32:55.660240Z","iopub.execute_input":"2023-12-07T16:32:55.660602Z","iopub.status.idle":"2023-12-07T16:32:55.669203Z","shell.execute_reply.started":"2023-12-07T16:32:55.660572Z","shell.execute_reply":"2023-12-07T16:32:55.668101Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([1.26367129, 1.25852342, 1.2609932 ])"},"metadata":{}}]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:32:55.670550Z","iopub.execute_input":"2023-12-07T16:32:55.670875Z","iopub.status.idle":"2023-12-07T16:32:55.688103Z","shell.execute_reply.started":"2023-12-07T16:32:55.670848Z","shell.execute_reply":"2023-12-07T16:32:55.686926Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.263671\n1  2222bbbb  1.258523\n2  4444cccc  1.260993","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.263671</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.258523</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.260993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"xgb_param={\n'reg_alpha': 0.0008774661176012108,\n'reg_lambda': 2.542812743920178,\n'colsample_bynode': 0.7839026197349153,\n'subsample': 0.8994226268096415,\n'eta': 0.04730766698056879, \n'max_depth': 3, \n'n_estimators': 1024,\n'random_state': 42,\n'eval_metric': 'rmse'\n}\n\nxgb_solution = XGBRegressor(**xgb_param)\nxgb_y_pred   = evaluate(x.copy(), y.copy(), xgb_solution, test_x=testin_x.copy()) ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:51:02.966390Z","iopub.execute_input":"2023-12-07T16:51:02.966802Z","iopub.status.idle":"2023-12-07T16:51:34.795311Z","shell.execute_reply.started":"2023-12-07T16:51:02.966772Z","shell.execute_reply":"2023-12-07T16:51:34.794093Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"xgb_sub = pd.DataFrame({'id': test_ids, 'score': xgb_y_pred})\n#xgb_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:51:34.797353Z","iopub.execute_input":"2023-12-07T16:51:34.797724Z","iopub.status.idle":"2023-12-07T16:51:34.803556Z","shell.execute_reply.started":"2023-12-07T16:51:34.797693Z","shell.execute_reply":"2023-12-07T16:51:34.802366Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"xgb_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:51:34.807692Z","iopub.execute_input":"2023-12-07T16:51:34.808107Z","iopub.status.idle":"2023-12-07T16:51:34.820629Z","shell.execute_reply.started":"2023-12-07T16:51:34.808074Z","shell.execute_reply":"2023-12-07T16:51:34.819483Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  2.716687\n1  2222bbbb  1.442105\n2  4444cccc  1.455410","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2.716687</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.442105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.455410</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# best_sc = 1\n# for w in np.arange(0, 1.01, 0.001):\n#     sc = metrics.mean_squared_error(x.copy(), \n#                                     w *  valid_predictions + (1-w) * valid_predictions, \n#                                     squared=False)\n#     if sc < best_sc:\n#         best_sc = sc\n#         best_w = w\n        \n# print('Composition RMSE score = {:.5f}'.format(best_sc))\n# print('Composition best W = {:.3f}'.format(best_w))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_w =0.81 # 밑 주석 처리된 부분에서 최적의 best_w  blending으로 도출 \n\nW = [best_w, 1 - best_w]\nprint(W)\ntest_preds = y_pred * W[0] + xgb_y_pred * W[1]\ntest_preds","metadata":{"execution":{"iopub.status.busy":"2023-12-07T17:19:45.823777Z","iopub.execute_input":"2023-12-07T17:19:45.824179Z","iopub.status.idle":"2023-12-07T17:19:45.833101Z","shell.execute_reply.started":"2023-12-07T17:19:45.824133Z","shell.execute_reply":"2023-12-07T17:19:45.831904Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[0.81, 0.18999999999999995]\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"array([1.53974419, 1.29340399, 1.29793248])"},"metadata":{}}]},{"cell_type":"code","source":"ensemble_sub = pd.DataFrame({'id': test_ids, 'score': test_preds})\nensemble_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T17:20:02.229313Z","iopub.execute_input":"2023-12-07T17:20:02.230478Z","iopub.status.idle":"2023-12-07T17:20:02.237700Z","shell.execute_reply.started":"2023-12-07T17:20:02.230435Z","shell.execute_reply":"2023-12-07T17:20:02.236327Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"ensemble_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-07T17:20:09.020051Z","iopub.execute_input":"2023-12-07T17:20:09.020482Z","iopub.status.idle":"2023-12-07T17:20:09.030327Z","shell.execute_reply.started":"2023-12-07T17:20:09.020449Z","shell.execute_reply":"2023-12-07T17:20:09.029496Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.539744\n1  2222bbbb  1.293404\n2  4444cccc  1.297932","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.539744</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.293404</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.297932</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def train_valid_split(data_x, data_y, train_idx, valid_idx):\n#     x_train = data_x.iloc[train_idx]\n#     y_train = data_y[train_idx]\n#     x_valid = data_x.iloc[valid_idx]\n#     y_valid = data_y[valid_idx]\n#     return x_train, y_train, x_valid, y_valid\n\n# # LightGBM 매개변수 (예시 매개변수)\n# lgbm_param = {\n#     'n_estimators': 1024,\n#     'learning_rate': 0.005,\n#     'metric': 'rmse',\n#     'random_state': 42\n# }\n\n# # XGBoost 매개변수\n# xgb_param = {\n#     'reg_alpha': 0.0008774661176012108,\n#     'reg_lambda': 2.542812743920178,\n#     'colsample_bynode': 0.7839026197349153,\n#     'subsample': 0.8994226268096415,\n#     'eta': 0.04730766698056879, \n#     'max_depth': 3, \n#     'n_estimators': 1024,\n#     'random_state': 42,\n#     'eval_metric': 'rmse'\n# }\n\n# # 모델 초기화\n# lgbm_model = LGBMRegressor(**lgbm_param)\n# xgb_model = XGBRegressor(**xgb_param)\n\n# # 교차 검증을 위한 설정\n# skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n# lgbm_preds = []\n# xgb_preds = []\n# valid_y_list = []\n\n# for train_index, valid_index in skf.split(x, y.astype(str)):\n#     # 훈련 및 검증 데이터 분리\n#     train_x, train_y, valid_x, valid_y = train_valid_split(x, y, train_index, valid_index)\n\n#     # LightGBM 모델 훈련 및 예측\n#     lgbm_model.fit(train_x, train_y)\n#     lgbm_preds.append(lgbm_model.predict(valid_x))\n\n#     # XGBoost 모델 훈련 및 예측\n#     xgb_model.fit(train_x, train_y)\n#     xgb_preds.append(xgb_model.predict(valid_x))\n\n#     # 검증 타겟 저장\n#     valid_y_list.append(valid_y)\n\n# # 모든 fold의 예측값과 실제값 통합\n# lgbm_preds = np.concatenate(lgbm_preds)\n# xgb_preds = np.concatenate(xgb_preds)\n# valid_y_combined = np.concatenate(valid_y_list)\n\n# # 최적 가중치 찾기\n# best_sc = float('inf')\n# best_w = 0\n# for w in np.arange(0, 1.01, 0.01):\n#     combined_pred = w * lgbm_preds + (1-w) * xgb_preds\n#     sc = mean_squared_error(valid_y_combined, combined_pred, squared=False)\n#     if sc < best_sc:\n#         best_sc = sc\n#         best_w = w\n\n# print('최적의 RMSE 점수 = {:.5f}'.format(best_sc))\n# print('최적의 가중치 W = {:.3f}'.format(best_w))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T17:16:25.901504Z","iopub.execute_input":"2023-12-07T17:16:25.901927Z","iopub.status.idle":"2023-12-07T17:18:06.619660Z","shell.execute_reply.started":"2023-12-07T17:16:25.901897Z","shell.execute_reply":"2023-12-07T17:18:06.618162Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"최적의 RMSE 점수 = 0.61579\n최적의 가중치 W = 0.810\n","output_type":"stream"}]}]}
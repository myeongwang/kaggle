{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b2f833",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-31T11:21:58.813114Z",
     "iopub.status.busy": "2023-12-31T11:21:58.812644Z",
     "iopub.status.idle": "2023-12-31T11:22:02.418322Z",
     "shell.execute_reply": "2023-12-31T11:22:02.417134Z"
    },
    "papermill": {
     "duration": 3.617375,
     "end_time": "2023-12-31T11:22:02.421129",
     "exception": false,
     "start_time": "2023-12-31T11:21:58.803754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3e259c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:22:02.437333Z",
     "iopub.status.busy": "2023-12-31T11:22:02.436916Z",
     "iopub.status.idle": "2023-12-31T11:22:02.506377Z",
     "shell.execute_reply": "2023-12-31T11:22:02.505266Z"
    },
    "papermill": {
     "duration": 0.080919,
     "end_time": "2023-12-31T11:22:02.509153",
     "exception": false,
     "start_time": "2023-12-31T11:22:02.428234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "# time_diff 정규화 피처 추가 \n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "#                              input_word_length_more6 = pl.col('text_change').apply(lambda x: sum(1 for i in x if len(i) >= 6) if len(x) > 0 else 0),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_less5_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x if len(i) < 5] if len(x) > 0 else [0])),\n",
    "                             \n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_4std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0)),\n",
    "\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_4skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0))\n",
    "                             )\n",
    "\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    # 유휴 시간 특징 추출\n",
    "    print(\"< Idle time features(normalized) >\")\n",
    "    \n",
    "    \n",
    "    # 이전 'up_time' 값을 현재 행으로 이동하고, 'up_time_lagged'라는 새로운 열로 저장\n",
    "    df = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    # 'down_time'과 'up_time_lagged'의 차이를 계산하여 'time_diff' 열 생성\n",
    "    df = df.with_columns((pl.col('down_time') - pl.col('up_time_lagged')).fill_null(0).alias('time_diff'))\n",
    "    # 'time_diff'의 평균 계산 및 정규화\n",
    "    time_diff_mean = df.select(pl.mean('time_diff')).collect().get_column('time_diff')[0]\n",
    "    df = df.with_columns((pl.col('time_diff') / time_diff_mean).alias('time_diff'))\n",
    "    # 필터링 및 통계치 계산\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(\n",
    "        inter_key_largest_lantency = pl.max('time_diff'),\n",
    "        inter_key_median_lantency = pl.median('time_diff'),\n",
    "        mean_pause_time = pl.mean('time_diff'),\n",
    "        std_pause_time = pl.std('time_diff'),\n",
    "        total_pause_time = pl.sum('time_diff'),\n",
    "        pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "        pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "        pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "        pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "        pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),\n",
    "    )\n",
    "    \n",
    "    # 계산된 통계치를 원래의 'feats' 데이터프레임에 병합\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    # P-버스트 특징 추출\n",
    "    print(\"< P-bursts features(normalized) >\")\n",
    "    # 위와 같은 방법으로 'up_time'을 이전 행으로 이동\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    # 'down_time'과 'up_time_lagged'의 차이를 초 단위로 계산\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링하고, 시간 차이가 2초 미만인 경우만 선택\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff') < 2)\n",
    "    # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'P-bursts'로 계산\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    # 결측값 제거\n",
    "    temp = temp.drop_nulls()\n",
    "    # 다양한 'P-bursts' 통계치 계산\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "\n",
    "    # 계산된 'P-bursts' 통계치를 'feats' 데이터프레임에 병합\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    # R-버스트 특징 추출\n",
    "    print(\"< R-bursts features(normalized) >\")\n",
    "    # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    # 'activity' 열이 'Remove/Cut'인 경우만 선택\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'R-bursts'로 계산\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    # 결측값 제거\n",
    "    temp = temp.drop_nulls()\n",
    "    # 다양한 'R-bursts' 통계치 계산\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "\n",
    "    # 계산된 'R-bursts' 통계치를 'feats' 데이터프레임에 병합\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    #######\n",
    "    print(\"< Normalized Delta Features >\")\n",
    "    # 각 이벤트의 시간 범위 계산\n",
    "    temp_delta = df.with_columns((pl.col('down_time').shift(-1) - pl.col('up_time')).alias('delta_e'))\n",
    "\n",
    "    # 복사 작업의 평균 시간 범위 계산\n",
    "    # 'delta_e' 열의 평균을 계산하고 결과 열의 이름을 명시적으로 지정합니다.\n",
    "    delta_ecopy = temp_delta.select(pl.mean('delta_e').alias('delta_e_mean')).collect().get_column('delta_e_mean')[0]\n",
    "\n",
    "    # 정규화 수행\n",
    "    temp_delta = temp_delta.with_columns((pl.col('delta_e') / delta_ecopy).alias('normalized_delta'))\n",
    "\n",
    "    # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n",
    "    temp_delta_agg = temp_delta.groupby(\"id\").agg(pl.mean('normalized_delta').suffix('_mean'), \n",
    "                                                  pl.std('normalized_delta').suffix('_std'))\n",
    "    feats = feats.join(temp_delta_agg, on='id', how='left')\n",
    "    \n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "        model.fit(train_x, train_y)\n",
    "        if test_x is None:\n",
    "            test_y[valid_index] = model.predict(valid_x)\n",
    "        else:\n",
    "            test_y[:, i] = model.predict(test_x)\n",
    "    return test_y if (test_x is None) else np.mean(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0bfd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:22:02.524657Z",
     "iopub.status.busy": "2023-12-31T11:22:02.524247Z",
     "iopub.status.idle": "2023-12-31T11:22:02.561820Z",
     "shell.execute_reply": "2023-12-31T11:22:02.560765Z"
    },
    "papermill": {
     "duration": 0.048637,
     "end_time": "2023-12-31T11:22:02.564569",
     "exception": false,
     "start_time": "2023-12-31T11:22:02.515932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c00556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:22:02.580495Z",
     "iopub.status.busy": "2023-12-31T11:22:02.580064Z",
     "iopub.status.idle": "2023-12-31T11:25:01.888215Z",
     "shell.execute_reply": "2023-12-31T11:25:01.887203Z"
    },
    "papermill": {
     "duration": 179.319204,
     "end_time": "2023-12-31T11:25:01.891085",
     "exception": false,
     "start_time": "2023-12-31T11:22:02.571881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features(normalized) >\n",
      "< P-bursts features(normalized) >\n",
      "< R-bursts features(normalized) >\n",
      "< Normalized Delta Features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 185\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features(normalized) >\n",
      "< P-bursts features(normalized) >\n",
      "< R-bursts features(normalized) >\n",
      "< Normalized Delta Features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Learning and Evaluation >\n"
     ]
    }
   ],
   "source": [
    "data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "train_feats['pw'] = train_feats['paragraph_len_sum']/train_feats['word_len_sum']\n",
    "train_feats              = train_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\n",
    "train_feats              = train_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "test_feats['pw'] = test_feats['paragraph_len_sum']/test_feats['word_len_sum']\n",
    "test_feats              = test_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\n",
    "test_feats              = test_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n",
    "\n",
    "test_ids = test_feats['id'].values\n",
    "testin_x = test_feats.drop(['id'], axis=1)\n",
    "\n",
    "print('< Learning and Evaluation >')\n",
    "param = {'n_estimators': 1024,\n",
    "         'learning_rate': 0.005,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': 42,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 0,}\n",
    "solution = LGBMRegressor(**param)\n",
    "y_pred   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f98dccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:01.908523Z",
     "iopub.status.busy": "2023-12-31T11:25:01.907357Z",
     "iopub.status.idle": "2023-12-31T11:25:01.927102Z",
     "shell.execute_reply": "2023-12-31T11:25:01.926179Z"
    },
    "papermill": {
     "duration": 0.030713,
     "end_time": "2023-12-31T11:25:01.929427",
     "exception": false,
     "start_time": "2023-12-31T11:25:01.898714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.248965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1.205973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1.254234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.248965\n",
       "1  2222bbbb  1.205973\n",
       "2  4444cccc  1.254234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame({'id': test_ids, 'score': y_pred})\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7a1955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:01.946532Z",
     "iopub.status.busy": "2023-12-31T11:25:01.945865Z",
     "iopub.status.idle": "2023-12-31T11:25:01.950731Z",
     "shell.execute_reply": "2023-12-31T11:25:01.949510Z"
    },
    "papermill": {
     "duration": 0.015962,
     "end_time": "2023-12-31T11:25:01.953064",
     "exception": false,
     "start_time": "2023-12-31T11:25:01.937102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 정규화된 time_diff 적용한 값\n",
    "\n",
    "# id\tscore\n",
    "# 0\t0000aaaa\t1.257917\n",
    "# 1\t2222bbbb\t1.205639\n",
    "# 2\t4444cccc\t1.214414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890893fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:01.970899Z",
     "iopub.status.busy": "2023-12-31T11:25:01.970067Z",
     "iopub.status.idle": "2023-12-31T11:25:01.985981Z",
     "shell.execute_reply": "2023-12-31T11:25:01.984884Z"
    },
    "papermill": {
     "duration": 0.027781,
     "end_time": "2023-12-31T11:25:01.988454",
     "exception": false,
     "start_time": "2023-12-31T11:25:01.960673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # time_diff 정규화 피처 추가 \n",
    "# def dev_feats(df):\n",
    "    \n",
    "#     print(\"< Count by values features >\")\n",
    "    \n",
    "#     feats = count_by_values(df, 'activity', activities)\n",
    "#     feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "#     print(\"< Input words stats features >\")\n",
    "\n",
    "#     temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "#     temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "#     temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "#                              input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "#     temp = temp.drop('text_change')\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "#     print(\"< Numerical columns features >\")\n",
    "\n",
    "#     temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "#                                  pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "#                                  pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< Categorical columns features >\")\n",
    "    \n",
    "#     temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "#     # 유휴 시간 특징 추출\n",
    "#     print(\"< Idle time features(normalized) >\")\n",
    "#     # 이전 'up_time' 값을 현재 행으로 이동하고, 'up_time_lagged'라는 새로운 열로 저장\n",
    "#     df = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     # 'down_time'과 'up_time_lagged'의 차이를 계산하여 'time_diff' 열 생성\n",
    "#     df = df.with_columns((pl.col('down_time') - pl.col('up_time_lagged')).fill_null(0).alias('time_diff'))\n",
    "#     # 'time_diff'의 평균 계산 및 정규화\n",
    "#     time_diff_mean = df.select(pl.mean('time_diff')).collect().get_column('time_diff')[0]\n",
    "#     df = df.with_columns((pl.col('time_diff') / time_diff_mean).alias('time_diff'))\n",
    "#     # 필터링 및 통계치 계산\n",
    "#     temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.group_by(\"id\").agg(\n",
    "#         inter_key_largest_lantency = pl.max('time_diff'),\n",
    "#         inter_key_median_lantency = pl.median('time_diff'),\n",
    "#         mean_pause_time = pl.mean('time_diff'),\n",
    "#         std_pause_time = pl.std('time_diff'),\n",
    "#         total_pause_time = pl.sum('time_diff'),\n",
    "#         pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#         pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#         pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#         pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#         pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),\n",
    "#     )\n",
    "    \n",
    "#     # 계산된 통계치를 원래의 'feats' 데이터프레임에 병합\n",
    "#     feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "#     # P-버스트 특징 추출\n",
    "#     print(\"< P-bursts features(normalized) >\")\n",
    "#     # 위와 같은 방법으로 'up_time'을 이전 행으로 이동\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     # 'down_time'과 'up_time_lagged'의 차이를 초 단위로 계산\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링하고, 시간 차이가 2초 미만인 경우만 선택\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('time_diff') < 2)\n",
    "#     # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'P-bursts'로 계산\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "#     # 결측값 제거\n",
    "#     temp = temp.drop_nulls()\n",
    "#     # 다양한 'P-bursts' 통계치 계산\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "#                                    pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "#                                    pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "\n",
    "#     # 계산된 'P-bursts' 통계치를 'feats' 데이터프레임에 병합\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "#     # R-버스트 특징 추출\n",
    "#     print(\"< R-bursts features(normalized) >\")\n",
    "#     # 'activity' 열이 'Input' 또는 'Remove/Cut'인 행만 필터링\n",
    "#     temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     # 'activity' 열이 'Remove/Cut'인 경우만 선택\n",
    "#     temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     # 연속된 행에서 같은 조건을 충족하는 경우의 수를 'R-bursts'로 계산\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "#     # 결측값 제거\n",
    "#     temp = temp.drop_nulls()\n",
    "#     # 다양한 'R-bursts' 통계치 계산\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "#                                    pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "#                                    pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "\n",
    "#     # 계산된 'R-bursts' 통계치를 'feats' 데이터프레임에 병합\n",
    "#     feats = feats.join(temp, on='id', how='left')\n",
    "#     #######\n",
    "#     print(\"< Normalized Delta Features >\")\n",
    "#     # 각 이벤트의 시간 범위 계산\n",
    "#     temp_delta = df.with_columns((pl.col('down_time').shift(-1) - pl.col('up_time')).alias('delta_e'))\n",
    "\n",
    "#     # 복사 작업의 평균 시간 범위 계산\n",
    "#     # 'delta_e' 열의 평균을 계산하고 결과 열의 이름을 명시적으로 지정합니다.\n",
    "#     delta_ecopy = temp_delta.select(pl.mean('delta_e').alias('delta_e_mean')).collect().get_column('delta_e_mean')[0]\n",
    "\n",
    "#     # 정규화 수행\n",
    "#     temp_delta = temp_delta.with_columns((pl.col('delta_e') / delta_ecopy).alias('normalized_delta'))\n",
    "\n",
    "#     # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n",
    "#     temp_delta_agg = temp_delta.groupby(\"id\").agg(pl.mean('normalized_delta').suffix('_mean'), \n",
    "#                                                   pl.std('normalized_delta').suffix('_std'))\n",
    "#     feats = feats.join(temp_delta_agg, on='id', how='left')\n",
    "    \n",
    "    \n",
    "#     print(\"< Idle time features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "#                                    inter_key_median_lantency = pl.median('time_diff'),\n",
    "#                                    mean_pause_time = pl.mean('time_diff'),\n",
    "#                                    std_pause_time = pl.std('time_diff'),\n",
    "#                                    total_pause_time = pl.sum('time_diff'),\n",
    "#                                    pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#                                    pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#                                    pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#                                    pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#                                    pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "#     print(\"< P-bursts features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "#                                    pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "#                                    pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< R-bursts features >\")\n",
    "\n",
    "#     temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "#                                    pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "#                                    pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "#     return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bef455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:02.006583Z",
     "iopub.status.busy": "2023-12-31T11:25:02.006166Z",
     "iopub.status.idle": "2023-12-31T11:25:02.017261Z",
     "shell.execute_reply": "2023-12-31T11:25:02.015994Z"
    },
    "papermill": {
     "duration": 0.023559,
     "end_time": "2023-12-31T11:25:02.019828",
     "exception": false,
     "start_time": "2023-12-31T11:25:01.996269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # original silver bullet\n",
    "# def dev_feats(df):\n",
    "    \n",
    "#     print(\"< Count by values features >\")\n",
    "    \n",
    "#     feats = count_by_values(df, 'activity', activities)\n",
    "#     feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "#     print(\"< Input words stats features >\")\n",
    "\n",
    "#     temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "#     temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "#     temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "#                              input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "#     temp = temp.drop('text_change')\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "#     print(\"< Numerical columns features >\")\n",
    "\n",
    "#     temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "#                                  pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "#                                  pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< Categorical columns features >\")\n",
    "    \n",
    "#     temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "#     print(\"< Idle time features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "#                                    inter_key_median_lantency = pl.median('time_diff'),\n",
    "#                                    mean_pause_time = pl.mean('time_diff'),\n",
    "#                                    std_pause_time = pl.std('time_diff'),\n",
    "#                                    total_pause_time = pl.sum('time_diff'),\n",
    "#                                    pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#                                    pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#                                    pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#                                    pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#                                    pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "#     print(\"< P-bursts features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "#                                    pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "#                                    pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< R-bursts features >\")\n",
    "\n",
    "#     temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "#                                    pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "#                                    pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "#      print(\"< Normalized Delta Features >\")\n",
    "#     # 각 이벤트의 시간 범위 계산\n",
    "#     temp_delta = df.with_columns((pl.col('down_time').shift(-1) - pl.col('up_time')).alias('delta_e'))\n",
    "\n",
    "#     # 복사 작업의 평균 시간 범위 계산\n",
    "#     # 'delta_e' 열의 평균을 계산하고 결과 열의 이름을 명시적으로 지정합니다.\n",
    "#     delta_ecopy = temp_delta.select(pl.mean('delta_e').alias('delta_e_mean')).collect().get_column('delta_e_mean')[0]\n",
    "\n",
    "#     # 정규화 수행\n",
    "#     temp_delta = temp_delta.with_columns((pl.col('delta_e') / delta_ecopy).alias('normalized_delta'))\n",
    "\n",
    "#     # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n",
    "#     temp_delta_agg = temp_delta.groupby(\"id\").agg(pl.mean('normalized_delta').suffix('_mean'), \n",
    "#                                                   pl.std('normalized_delta').suffix('_std'))\n",
    "#     feats = feats.join(temp_delta_agg, on='id', how='left')\n",
    "#     return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc91f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:02.037930Z",
     "iopub.status.busy": "2023-12-31T11:25:02.037490Z",
     "iopub.status.idle": "2023-12-31T11:25:02.052512Z",
     "shell.execute_reply": "2023-12-31T11:25:02.051225Z"
    },
    "papermill": {
     "duration": 0.027205,
     "end_time": "2023-12-31T11:25:02.054995",
     "exception": false,
     "start_time": "2023-12-31T11:25:02.027790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # final \n",
    "\n",
    "# def dev_feats(df):\n",
    "#     print(\"< Count by values features >\")\n",
    "#     feats = count_by_values(df, 'activity', activities)\n",
    "#     feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "#     print(\"< Input words stats features >\")\n",
    "#     temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "\n",
    "#     # 'text_change' 열의 데이터 타입 확인 및 처리\n",
    "#     df_collected = df.collect()  # 데이터를 메모리에 로드\n",
    "#     text_change_dtype = df_collected['text_change'].dtype\n",
    "#     if text_change_dtype == pl.Utf8:\n",
    "#         temp = temp.with_columns(pl.col('text_change').str.lengths().alias('text_change_length'))\n",
    "#     else:\n",
    "#         temp = temp.with_columns(pl.col('text_change').list.lengths().alias('text_change_length'))\n",
    "\n",
    "    \n",
    "#     temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "#     temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "#                              input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "#     temp = temp.drop('text_change')\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "#     print(\"< Numerical columns features >\")\n",
    "#     temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "#                                  pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "#                                  pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "#     print(\"< Categorical columns features >\")\n",
    "#     temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "#     # 유휴 시간 특징 계산 (정규화되지 않은 time_diff 사용)\n",
    "#     print(\"< Idle time features (Original time_diff) >\")\n",
    "#     df_original = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     df_original = df_original.with_columns((pl.col('down_time') - pl.col('up_time_lagged')).fill_null(0).alias('time_diff'))\n",
    "#     temp_original = df_original.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp_original = temp_original.group_by(\"id\").agg(\n",
    "#         inter_key_largest_latency = pl.max('time_diff'),\n",
    "#         inter_key_median_latency = pl.median('time_diff'),\n",
    "#         mean_pause_time = pl.mean('time_diff'),\n",
    "#         std_pause_time = pl.std('time_diff'),\n",
    "#         total_pause_time = pl.sum('time_diff'),\n",
    "#         pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#         pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#         pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#         pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#         pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),\n",
    "#     )\n",
    "#     feats = feats.join(temp_original, on='id', how='left', suffix='_original')\n",
    "\n",
    "#     # 유휴 시간 특징 계산 (정규화된 time_diff 사용)\n",
    "#     print(\"< Idle time features (Normalized time_diff) >\")\n",
    "#     time_diff_mean = df_original.select(pl.mean('time_diff')).collect().get_column('time_diff')[0]\n",
    "#     df_normalized = df_original.with_columns((pl.col('time_diff') / time_diff_mean).alias('time_diff'))\n",
    "#     temp_normalized = df_normalized.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp_normalized = temp_normalized.group_by(\"id\").agg(\n",
    "#         inter_key_largest_latency = pl.max('time_diff'),\n",
    "#         inter_key_median_latency = pl.median('time_diff'),\n",
    "#         mean_pause_time = pl.mean('time_diff'),\n",
    "#         std_pause_time = pl.std('time_diff'),\n",
    "#         total_pause_time = pl.sum('time_diff'),\n",
    "#         pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#         pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#         pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#         pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#         pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),\n",
    "#     )\n",
    "#     feats = feats.join(temp_normalized, on='id', how='left', suffix='_normalized')\n",
    "    \n",
    "#         # P-bursts 및 R-bursts 특징 계산 (원래 및 정규화된 time_diff 모두 사용)\n",
    "#     # P-bursts 특징 계산 (정규화되지 않은 time_diff 사용)\n",
    "#     print(\"< P-bursts features (Original time_diff) >\")\n",
    "#     temp_pburst_original = df_original.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp_pburst_original = temp_pburst_original.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).alias('time_diff'))\n",
    "#     temp_pburst_original = temp_pburst_original.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp_pburst_original = temp_pburst_original.group_by(\"id\").agg(\n",
    "#         p_bursts_count = pl.count('time_diff').filter(pl.col('time_diff') < 2)\n",
    "#     )\n",
    "#     feats = feats.join(temp_pburst_original, on='id', how='left', suffix='_pburst_original')\n",
    "\n",
    "#     # P-bursts 특징 계산 (정규화된 time_diff 사용)\n",
    "#     print(\"< P-bursts features (Normalized time_diff) >\")\n",
    "#     temp_pburst_normalized = df_normalized.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp_pburst_normalized = temp_pburst_normalized.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).alias('time_diff'))\n",
    "#     temp_pburst_normalized = temp_pburst_normalized.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp_pburst_normalized = temp_pburst_normalized.group_by(\"id\").agg(\n",
    "#         p_bursts_count = pl.count('time_diff').filter(pl.col('time_diff') < 2)\n",
    "#     )\n",
    "#     feats = feats.join(temp_pburst_normalized, on='id', how='left', suffix='_pburst_normalized')\n",
    "\n",
    "#     # R-bursts 특징 계산 (정규화되지 않은 time_diff 사용)\n",
    "#     print(\"< R-bursts features (Original time_diff) >\")\n",
    "#     temp_rburst_original = df_original.filter(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     temp_rburst_original = temp_rburst_original.group_by(\"id\").agg(\n",
    "#         r_bursts_count = pl.count('activity')\n",
    "#     )\n",
    "#     feats = feats.join(temp_rburst_original, on='id', how='left', suffix='_rburst_original')\n",
    "\n",
    "#     # R-bursts 특징 계산 (정규화된 time_diff 사용)\n",
    "#     print(\"< R-bursts features (Normalized time_diff) >\")\n",
    "#     temp_rburst_normalized = df_normalized.filter(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     temp_rburst_normalized = temp_rburst_normalized.group_by(\"id\").agg(\n",
    "#         r_bursts_count = pl.count('activity')\n",
    "#     )\n",
    "#     feats = feats.join(temp_rburst_normalized, on='id', how='left', suffix='_rburst_normalized')\n",
    "    \n",
    "#     print(\"< Normalized Delta Features >\")\n",
    "#     # 각 이벤트의 시간 범위 계산\n",
    "#     temp_delta = df.with_columns((pl.col('down_time').shift(-1) - pl.col('up_time')).alias('delta_e'))\n",
    "\n",
    "#     # 복사 작업의 평균 시간 범위 계산\n",
    "#     # 'delta_e' 열의 평균을 계산하고 결과 열의 이름을 명시적으로 지정합니다.\n",
    "#     delta_ecopy = temp_delta.select(pl.mean('delta_e').alias('delta_e_mean')).collect().get_column('delta_e_mean')[0]\n",
    "\n",
    "#     # 정규화 수행\n",
    "#     temp_delta = temp_delta.with_columns((pl.col('delta_e') / delta_ecopy).alias('normalized_delta'))\n",
    "\n",
    "#     # 계산된 정규화된 특징을 'feats' 데이터프레임에 병합\n",
    "#     temp_delta_agg = temp_delta.groupby(\"id\").agg(pl.mean('normalized_delta').suffix('_mean'), \n",
    "#                                                   pl.std('normalized_delta').suffix('_std'))\n",
    "#     feats = feats.join(temp_delta_agg, on='id', how='left')\n",
    "\n",
    "#     return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beead654",
   "metadata": {
    "papermill": {
     "duration": 0.007806,
     "end_time": "2023-12-31T11:25:02.071404",
     "exception": false,
     "start_time": "2023-12-31T11:25:02.063598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## hb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a472dda1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:02.089604Z",
     "iopub.status.busy": "2023-12-31T11:25:02.089182Z",
     "iopub.status.idle": "2023-12-31T11:25:02.101863Z",
     "shell.execute_reply": "2023-12-31T11:25:02.100650Z"
    },
    "papermill": {
     "duration": 0.02525,
     "end_time": "2023-12-31T11:25:02.104310",
     "exception": false,
     "start_time": "2023-12-31T11:25:02.079060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "# activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "# events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "# text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "# def count_by_values(df, colname, values):\n",
    "#     fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "#     for i, value in enumerate(values):\n",
    "#         tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "#         fts  = fts.join(tmp_df, on='id', how='left') \n",
    "#     return fts\n",
    "\n",
    "# ###\n",
    "# def count_words(texts):\n",
    "#     word_count = 0\n",
    "#     for text in texts:\n",
    "#         for word in text.split():\n",
    "#             if len(word) >= 4:\n",
    "#                 word_count += 1\n",
    "#     return word_count\n",
    "\n",
    "# def dev_feats(df):\n",
    "    \n",
    "#     print(\"< Count by values features >\")\n",
    "    \n",
    "#     feats = count_by_values(df, 'activity', activities)\n",
    "#     feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "#     feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "#     print(\"< Input words stats features >\")\n",
    "\n",
    "#     temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "#     temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "#     temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "# #                              input_word_length_more6 = pl.col('text_change').apply(lambda x: sum(1 for i in x if len(i) >= 6) if len(x) > 0 else 0),\n",
    "#                              input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "# #                              input_word_length_less5_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x if len(i) < 5] if len(x) > 0 else [0])),\n",
    "                             \n",
    "#                              input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "#                              input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "# #                              input_word_length_4std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0)),\n",
    "\n",
    "#                              input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "\n",
    "#                              input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "# #                              input_word_length_4skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x if len(i) > 4] if len(x) > 4 else 0))\n",
    "#                              )\n",
    "\n",
    "#     temp = temp.drop('text_change')\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "#     print(\"< Numerical columns features >\")\n",
    "\n",
    "#     temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "#                                  pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "#                                  pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< Categorical columns features >\")\n",
    "    \n",
    "#     temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "#     print(\"< Idle time features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "#                                    inter_key_median_lantency = pl.median('time_diff'),\n",
    "#                                    mean_pause_time = pl.mean('time_diff'),\n",
    "#                                    std_pause_time = pl.std('time_diff'),\n",
    "#                                    total_pause_time = pl.sum('time_diff'),\n",
    "#                                    pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "#                                    pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "#                                    pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "#                                    pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "#                                    pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "#     print(\"< P-bursts features >\")\n",
    "\n",
    "#     temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "#     temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "#     temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "#                                    pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "#                                    pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "#     print(\"< R-bursts features >\")\n",
    "\n",
    "#     temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "#     temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "#     temp = temp.drop_nulls()\n",
    "#     temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "#                                    pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "#                                    pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "#     feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "#     return feats\n",
    "\n",
    "\n",
    "# def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "#     x_train = data_x.iloc[train_idx]\n",
    "#     y_train = data_y[train_idx]\n",
    "#     x_valid = data_x.iloc[valid_idx]\n",
    "#     y_valid = data_y[valid_idx]\n",
    "#     return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "# def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "#     skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "#     test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n",
    "#     for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "#         train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "#         model.fit(train_x, train_y)\n",
    "#         if test_x is None:\n",
    "#             test_y[valid_index] = model.predict(valid_x)\n",
    "#         else:\n",
    "#             test_y[:, i] = model.predict(test_x)\n",
    "#     return test_y if (test_x is None) else np.mean(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68c7b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:02.122222Z",
     "iopub.status.busy": "2023-12-31T11:25:02.121375Z",
     "iopub.status.idle": "2023-12-31T11:25:02.134123Z",
     "shell.execute_reply": "2023-12-31T11:25:02.132980Z"
    },
    "papermill": {
     "duration": 0.024548,
     "end_time": "2023-12-31T11:25:02.136572",
     "exception": false,
     "start_time": "2023-12-31T11:25:02.112024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def q1(x):\n",
    "#     return x.quantile(0.25)\n",
    "# def q3(x):\n",
    "#     return x.quantile(0.75)\n",
    "\n",
    "# AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "# AGGREGATIONS_w = ['count', 'mean', 'min', 'max', 'first', 'last','sem','skew', q1, 'median', q3, 'sum']\n",
    "\n",
    "# def reconstruct_essay(currTextInput):\n",
    "#     essayText = \"\"\n",
    "#     for Input in currTextInput.values:\n",
    "#         if Input[0] == 'Replace':\n",
    "#             replaceTxt = Input[2].split(' => ')\n",
    "#             essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "#             continue\n",
    "#         if Input[0] == 'Paste':\n",
    "#             essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "#             continue\n",
    "#         if Input[0] == 'Remove/Cut':\n",
    "#             essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "#             continue\n",
    "#         if \"M\" in Input[0]:\n",
    "#             croppedTxt = Input[0][10:]\n",
    "#             splitTxt = croppedTxt.split(' To ')\n",
    "#             valueArr = [item.split(', ') for item in splitTxt]\n",
    "#             moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "#             if moveData[0] != moveData[2]:\n",
    "#                 if moveData[0] < moveData[2]:\n",
    "#                     essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "#                 else:\n",
    "#                     essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "#             continue\n",
    "#         essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "#     return essayText\n",
    "\n",
    "\n",
    "# def get_essay_df(df):\n",
    "#     df       = df[df.activity != 'Nonproduction']\n",
    "#     temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "#     essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "#     essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "#     return essay_df\n",
    "\n",
    "\n",
    "# def word_feats(df):\n",
    "#     essay_df = df\n",
    "#     df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "#     df = df.explode('word')\n",
    "#     df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "\n",
    "#     df = df[df['word_len'] != 0]\n",
    "\n",
    "#     word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS_w)\n",
    "\n",
    "#     word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "#     word_agg_df['id'] = word_agg_df.index\n",
    "#     word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "#     return word_agg_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def sent_feats(df):\n",
    "#     df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "#     df = df.explode('sent')\n",
    "#     df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "#     # Number of characters in sentences\n",
    "#     df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "#     # Number of words in sentences\n",
    "#     df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "#     df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "#     sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "#                              df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "#     sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "#     sent_agg_df['id'] = sent_agg_df.index\n",
    "#     sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "#     sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "#     sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "#     return sent_agg_df\n",
    "\n",
    "\n",
    "# def parag_feats(df):\n",
    "#     df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "#     df = df.explode('paragraph')\n",
    "#     # Number of characters in paragraphs\n",
    "#     df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "#     # Number of words in paragraphs\n",
    "#     df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "#     df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "#     paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "#                                   df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "#     paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "#     paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "#     paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "#     paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "#     paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "#     return paragraph_agg_df\n",
    "\n",
    "# def product_to_keys(logs, essays):\n",
    "#     essays['product_len'] = essays.essay.str.len()\n",
    "#     tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "#     essays = essays.merge(tmp_df, on='id', how='left')\n",
    "#     essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "#     return essays[['id', 'product_to_keys']]\n",
    "\n",
    "# def get_keys_pressed_per_second(logs):\n",
    "#     temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "#     temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "#     temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "#     temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "#     return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f43f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T11:25:02.155402Z",
     "iopub.status.busy": "2023-12-31T11:25:02.154596Z",
     "iopub.status.idle": "2023-12-31T11:25:02.164814Z",
     "shell.execute_reply": "2023-12-31T11:25:02.163824Z"
    },
    "papermill": {
     "duration": 0.022269,
     "end_time": "2023-12-31T11:25:02.167476",
     "exception": false,
     "start_time": "2023-12-31T11:25:02.145207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "# train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "# train_feats   = dev_feats(train_logs)\n",
    "# train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "# print('< Essay Reconstruction >')\n",
    "# train_logs             = train_logs.collect().to_pandas()\n",
    "# train_essays           = get_essay_df(train_logs)\n",
    "# train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "\n",
    "# train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "# train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "# train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "# train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "# # train_feats['pw'] = train_feats['']/train_feats['']\n",
    "# # train_feats['pw'] = train_feats['paragraph_word_count_sum']- train_feats['word_len_count']\n",
    "# train_feats['pw'] = train_feats['paragraph_len_sum']/train_feats['word_len_sum']\n",
    "# # train_feats['input_word_ratio'] = train_feats['input_word_count']/train_feats['input_word_length_more6']\n",
    "# # train_feats['pw'] = train_feats['down_event_0_cnt']/train_feats['down_event_1_cnt']\n",
    "# # train_feats['paragraph_first_last'] = train_feats['paragraph_len_first'] + train_feats['paragraph_len_last']\n",
    "\n",
    "# train_feats              = train_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\n",
    "# train_feats              = train_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n",
    "\n",
    "# # train_feats              = train_feats.drop(['input_word_length_more6'], axis=1)\n",
    "# # train_feats              = train_feats.drop(['word_count_min', 'cursor_position_min', 'action_time_min'], axis=1)\n",
    "# # train_feats              = train_feats.drop(['up_time_std','up_time_mean','up_time_median', 'up_time_min', 'up_time_max', 'up_time_quantile'], axis=1)\n",
    "\n",
    "\n",
    "# print('< Mapping >')\n",
    "# train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "# data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "# x              = data.drop(['id', 'score'], axis=1)\n",
    "# y              = data['score'].values\n",
    "# print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "# print('< Testing Data >')\n",
    "# test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "# test_feats  = dev_feats(test_logs)\n",
    "# test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "# test_logs             = test_logs.collect().to_pandas()\n",
    "# test_essays           = get_essay_df(test_logs)\n",
    "# test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "\n",
    "# test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "# test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "# test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "# test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "\n",
    "# # test_feats['pw'] = test_feats['']/test_feats['']\n",
    "# # test_feats['pw'] = test_feats['paragraph_word_count_sum']-test_feats['word_len_count']\n",
    "# test_feats['pw'] = test_feats['paragraph_len_sum']/test_feats['word_len_sum']\n",
    "# # test_feats['input_word_ratio'] = test_feats['input_word_count']/test_feats['input_word_length_more6']\n",
    "# # test_feats['pw'] = test_feats['down_event_0_cnt']/test_feats['down_event_1_cnt']\n",
    "# # test_feats['paragraph_first_last'] = test_feats['paragraph_len_first'] + test_feats['paragraph_len_last']\n",
    "# # test_feats              = test_feats.drop(['input_word_length_more6'], axis=1)\n",
    "# test_feats              = test_feats.drop(['word_len_median', 'cursor_position_min','paragraph_len_sum'], axis=1)\n",
    "# test_feats              = test_feats.drop(['down_event_10_cnt', 'down_event_11_cnt','down_event_12_cnt'], axis=1)\n",
    "\n",
    "# # test_feats              = test_feats.drop(['word_count_min', 'cursor_position_min', 'action_time_min'], axis=1)\n",
    "# # 'word_len_median', 'input_word_length_median', 'down_event_15_cnt', 'word_len_min', 'sent_word_count_sum'\n",
    "# # test_feats              = test_feats.drop(['up_time_std','up_time_mean','up_time_median', 'up_time_min', 'up_time_max', 'up_time_quantile'], axis=1)\n",
    "\n",
    "\n",
    "# test_ids = test_feats['id'].values\n",
    "# testin_x = test_feats.drop(['id'], axis=1)\n",
    "\n",
    "# print('< Learning and Evaluation >')\n",
    "# param = {'n_estimators': 1024,\n",
    "#          'learning_rate': 0.008,\n",
    "#          'metric': 'rmse',\n",
    "#          'random_state': 42,\n",
    "#          'force_col_wise': True,\n",
    "#          'verbosity': 0,}\n",
    "# solution = LGBMRegressor(**param)\n",
    "# y_pred   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n",
    "\n",
    "# sub = pd.DataFrame({'id': test_ids, 'score': y_pred})\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 188.155548,
   "end_time": "2023-12-31T11:25:03.300276",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-31T11:21:55.144728",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport ctypes\nimport os\nimport itertools\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport pprint\nimport time\nimport copy\nimport lightgbm as lgb\nimport torch\nimport polars as pl\nimport optuna\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression,Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\n%matplotlib inline\nfrom random import choice, choices\nfrom functools import reduce, partial\nfrom tqdm import tqdm\nfrom itertools import cycle\nfrom collections import Counter\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nfrom transformers import BertTokenizer\nfrom collections import Counter, defaultdict\nfrom tqdm.autonotebook import tqdm\nfrom math import sqrt\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-16T06:58:32.670288Z","iopub.execute_input":"2023-12-16T06:58:32.670664Z","iopub.status.idle":"2023-12-16T06:58:32.892782Z","shell.execute_reply.started":"2023-12-16T06:58:32.670634Z","shell.execute_reply":"2023-12-16T06:58:32.891262Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef getEssays(df):\n  \n    # 'id', 'activity', 'cursor_position', 'text_change' 열만 선택한 DataFrame 복사\n    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    \n    # 'activity' 열에서 'Nonproduction'인 행을 제외\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n\n    # 각 'id'별로 발생한 활동 수를 계산하여 배열로 저장\n    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n\n    lastIndex = 0\n\n    # 결과를 저장할 Pandas Series 생성\n    essaySeries = pd.Series()\n\n    for index, valCount in enumerate(valCountsArr):\n\n        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n        lastIndex += valCount\n        essayText = \"\"\n\n        for Input in currTextInput.values:\n            \n            # Input[0] = activity\n            # Input[2] = cursor_position\n            # Input[3] = text_change\n            \n            if Input[0] == 'Replace':\n                # '=>' 문자열을 기준으로 text_change를 분할\n                replaceTxt = Input[2].split(' => ')\n                \n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n\n                \n            # If activity = Paste    \n            if Input[0] == 'Paste':\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n\n                \n            # If activity = Remove/Cut\n            if Input[0] == 'Remove/Cut':\n                # DONT TOUCH\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n\n                \n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # \"Move from to\" 텍스트를 제거\n                croppedTxt = Input[0][10:]\n                \n                # ' To '를 기준으로 문자열을 분할\n                splitTxt = croppedTxt.split(' To ')\n                \n                # 문자열을 다시 ', '를 기준으로 분할하여 배열로 저장\n                valueArr = [item.split(', ') for item in splitTxt]\n                \n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n\n                # 같은 위치로 이동하는 경우 건너뛰기\n                if moveData[0] != moveData[2]:\n                    # 텍스트를 앞으로 이동시키는 경우 (다른 경우)\n                    if moveData[0] < moveData[2]:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    else:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue\n                         \n            # If just input\n            # DONT TOUCH\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n\n        # 결과 시리즈의 해당 인덱스에 에세이 텍스트를 설정  \n        essaySeries[index] = essayText\n     \n    # 결과 시리즈의 인덱스를 고유한 'id' 값으로 설정\n    essaySeries.index =  textInputDf['id'].unique()\n    \n    # 에세이 시리즈 반환\n    return essaySeries","metadata":{"execution":{"iopub.status.busy":"2023-12-16T06:58:32.894858Z","iopub.execute_input":"2023-12-16T06:58:32.895228Z","iopub.status.idle":"2023-12-16T06:58:32.911359Z","shell.execute_reply.started":"2023-12-16T06:58:32.895200Z","shell.execute_reply":"2023-12-16T06:58:32.910677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\ntrain_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\ntestdf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')\n\ntrain_essays = getEssays(traindf)\ntest_essays = getEssays(testdf)\n\ntrain_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\ntest_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})\n\nmerged_data = train_essaysdf.merge(train_scores, on='id')\n\n#단어와 단어의 연속된 조합(단어 n-그램)을 추출하고, 이를 기반으로 텍스트 데이터를 벡터 형태로 변환\n#X_tokenizer_train과 X_tokenizer_test에는 학습 및 테스트 데이터에 대한 단어 빈도 벡터가 저장\n#y는 target value score\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\nX_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\nX_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\ncount_vectorizer.get_feature_names_out() #ADDED\ny = merged_data['score']","metadata":{"execution":{"iopub.status.busy":"2023-12-16T06:58:32.912542Z","iopub.execute_input":"2023-12-16T06:58:32.913732Z","iopub.status.idle":"2023-12-16T07:07:29.040908Z","shell.execute_reply.started":"2023-12-16T06:58:32.913681Z","shell.execute_reply":"2023-12-16T07:07:29.039215Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame()\ndf_test = pd.DataFrame()\n\n# 메모리 요구사항 증가 but, 연산 수행시 효과적: dense 행렬 변환\nX_tokenizer_train = X_tokenizer_train.todense()\nX_tokenizer_test = X_tokenizer_test.todense()\n\nfor i in range(X_tokenizer_train.shape[1]) : \n    L = list(X_tokenizer_train[:,i])\n    li = [int(x) for x in L ]\n    df_train[f'feature {i}'] = li\n    \nfor i in range(X_tokenizer_test.shape[1]) : \n    L = list(X_tokenizer_test[:,i])\n    li = [int(x) for x in L ]\n    df_test[f'feature {i}'] = li    ","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:07:29.043044Z","iopub.execute_input":"2023-12-16T07:07:29.043357Z","iopub.status.idle":"2023-12-16T07:07:32.088303Z","shell.execute_reply.started":"2023-12-16T07:07:29.043335Z","shell.execute_reply":"2023-12-16T07:07:32.087291Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train_index = train_essaysdf['id']\ndf_test_index = test_essaysdf['id']\n\n# id 열 추가 \ndf_train.loc[:, 'id'] = df_train_index\ndf_test.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:07:32.089558Z","iopub.execute_input":"2023-12-16T07:07:32.089947Z","iopub.status.idle":"2023-12-16T07:07:32.099049Z","shell.execute_reply.started":"2023-12-16T07:07:32.089915Z","shell.execute_reply":"2023-12-16T07:07:32.097666Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# 집계함수 적용 df\ntrain_agg_fe_df = traindf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)\n\ntest_agg_fe_df = testdf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:07:32.100942Z","iopub.execute_input":"2023-12-16T07:07:32.101427Z","iopub.status.idle":"2023-12-16T07:07:36.885778Z","shell.execute_reply.started":"2023-12-16T07:07:32.101392Z","shell.execute_reply":"2023-12-16T07:07:36.884669Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nclass Preprocessor:\n    \n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n        \n        self.idf = defaultdict(float)\n#         self.gaps = [1, 2]\n    \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n\n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df):\n        \n        print(\"Starting to engineer features\")\n        \n        # initialize features dataframe\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        # get shifted features\n        # time shift\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # cursor position shift\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # word count shift\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # get aggregate statistical features\n        print(\"Engineering statistical summaries for features\")\n        # [(feature name, [ stat summaries to add ])]\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew'])\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                    \n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        # counts\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        # input words\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n\n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n        \n        print(\"Done!\")\n        return feats","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:07:36.887555Z","iopub.execute_input":"2023-12-16T07:07:36.889051Z","iopub.status.idle":"2023-12-16T07:07:36.930696Z","shell.execute_reply.started":"2023-12-16T07:07:36.888977Z","shell.execute_reply":"2023-12-16T07:07:36.929149Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 위 클래스 이용, train,test data -> feature engineering.\npreprocessor = Preprocessor(seed=42)\n\nprint(\"Engineering features for training data\")\n\nother_train_feats = preprocessor.make_feats(traindf)\n\nprint()\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\nprint(\"-\"*25)\nother_test_feats = preprocessor.make_feats(testdf)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:07:36.933053Z","iopub.execute_input":"2023-12-16T07:07:36.933368Z","iopub.status.idle":"2023-12-16T07:11:24.879081Z","shell.execute_reply.started":"2023-12-16T07:07:36.933340Z","shell.execute_reply":"2023-12-16T07:11:24.877487Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Engineering features for training data\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3485fef32ba645c385efd49f49c2a34e"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a260a14ee224d4d86b919854f4e158b"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08ae37db6d24582bba3cb89f2a9969f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753bc65fadde4842a9a684266dbb45b9"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6c2b12e3509464394a2a1a05c38bc60"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960542d02b584a6cb67c3f1e23550384"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n\n-------------------------\nEngineering features for test data\n-------------------------\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c54af6be6c47d89736e4025c1a047e"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5218ac2a3cd43648384ff3027605c9e"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b236c1ab4fa84ab69a4eb19b9925c475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a3f9aaa9a824daa80df7bd4c33f46fd"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ca451df7484fde8c57803298faefc8"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66e80d76f684fe0b4ca7159a5ced38f"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train_all = pd.DataFrame()\ndf_test_all = pd.DataFrame()\n\ndf_train_all = df_train.merge(train_agg_fe_df,on='id')\ndf_test_all = df_test.merge(test_agg_fe_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:24.881203Z","iopub.execute_input":"2023-12-16T07:11:24.881645Z","iopub.status.idle":"2023-12-16T07:11:24.906591Z","shell.execute_reply.started":"2023-12-16T07:11:24.881606Z","shell.execute_reply":"2023-12-16T07:11:24.905529Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 1,3 사분위 수 계산\ndef q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\nAGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', 'sum']\n\ndef split_essays_into_sentences(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n    essay_df = essay_df.explode('sent')\n    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n    return essay_df\n\ndef compute_sentence_aggregations(df):\n    sent_agg_df = pd.concat(\n        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    )\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\ndef split_essays_into_paragraphs(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n    essay_df = essay_df.explode('paragraph')\n    # Number of characters in paragraphs\n    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n    return essay_df\n\ndef compute_paragraph_aggregations(df):\n    paragraph_agg_df = pd.concat(\n        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    ) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:24.909928Z","iopub.execute_input":"2023-12-16T07:11:24.910326Z","iopub.status.idle":"2023-12-16T07:11:24.927527Z","shell.execute_reply.started":"2023-12-16T07:11:24.910298Z","shell.execute_reply":"2023-12-16T07:11:24.925712Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_sent_df = split_essays_into_sentences(train_essaysdf)\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n\ntrain_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n\ntest_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))\n\ntrain_paragraph_agg_df.loc[:, 'id'] = df_train_index\ntrain_sent_agg_df.loc[:, 'id'] = df_train_index\n\ntest_paragraph_agg_df.loc[:, 'id'] = df_test_index\ntest_sent_agg_df.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:24.929660Z","iopub.execute_input":"2023-12-16T07:11:24.929980Z","iopub.status.idle":"2023-12-16T07:11:35.283918Z","shell.execute_reply.started":"2023-12-16T07:11:24.929954Z","shell.execute_reply":"2023-12-16T07:11:35.282757Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"new_train_feats = pd.DataFrame()\nnew_test_feats = pd.DataFrame()\n\nnew_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\nnew_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')\n\nnew_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\nnew_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')\n\ntrain_feats = pd.DataFrame()\ntest_feats = pd.DataFrame()\n\ntrain_feats = new_train_feats.merge(other_train_feats,on='id')\ntest_feats = new_test_feats.merge(other_test_feats,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:35.285406Z","iopub.execute_input":"2023-12-16T07:11:35.285843Z","iopub.status.idle":"2023-12-16T07:11:35.641366Z","shell.execute_reply.started":"2023-12-16T07:11:35.285813Z","shell.execute_reply":"2023-12-16T07:11:35.640000Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#  키보드 입력에 관한 로그 데이터에서 시간 지연과 휴지 기간에 대한 통계를 계산하고, 이를 훈련 및 테스트 데이터 세트에 병합\n# train_feats와 test_feats라는 기존 피처 데이터 세트에 계산된 통계를 병합.\n# 추가적으로 train_feats는 train_scores와 병합하여 최종 훈련 데이터 세트 완성.\n\ndata = []\n\nfor logs in [traindf, testdf]:\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n\n    group = logs.groupby('id')['time_diff']\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n\n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:35.643232Z","iopub.execute_input":"2023-12-16T07:11:35.643548Z","iopub.status.idle":"2023-12-16T07:11:43.972811Z","shell.execute_reply.started":"2023-12-16T07:11:35.643523Z","shell.execute_reply":"2023-12-16T07:11:43.971827Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# train_feats 데이터프레임의 'score' 열을 정수형 레이블로 변환 ex) 3.5 -> 6(0.5 단위 씩)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_feats['score_class'] = le.fit_transform(train_feats['score'])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:43.974139Z","iopub.execute_input":"2023-12-16T07:11:43.974859Z","iopub.status.idle":"2023-12-16T07:11:43.980877Z","shell.execute_reply.started":"2023-12-16T07:11:43.974830Z","shell.execute_reply":"2023-12-16T07:11:43.979878Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"target_col = ['score']\n\ndrop_cols = ['id', 'score_class']\ntrain_cols = list()\n\ntrain_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:43.982181Z","iopub.execute_input":"2023-12-16T07:11:43.982488Z","iopub.status.idle":"2023-12-16T07:11:43.998356Z","shell.execute_reply.started":"2023-12-16T07:11:43.982466Z","shell.execute_reply":"2023-12-16T07:11:43.996222Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n\n# 결측치 최빈값으로 처리 \nfor col in nan_cols:\n    mode_value_train = train_feats[col].mode()[0] #최빈값 여러 개 일 경우 첫 번째꺼 선택 \n    train_feats[col].fillna(mode_value_train, inplace=True)\n    \nfor col in test_feats.columns[test_feats.isna().any()].tolist():\n    # Find the most frequent value in the training set for the current feature\n    most_frequent_value_train = train_feats[col].mode()[0]\n    \n    # Fill missing values in the test set with the most frequent value from the training set\n    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n\nprint(train_feats.shape, test_feats.shape)\nset(train_feats.columns) - set(test_feats.columns)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:44.000538Z","iopub.execute_input":"2023-12-16T07:11:44.000987Z","iopub.status.idle":"2023-12-16T07:11:44.142899Z","shell.execute_reply.started":"2023-12-16T07:11:44.000952Z","shell.execute_reply":"2023-12-16T07:11:44.141561Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"(2471, 659) (3, 657)\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'score', 'score_class'}"},"metadata":{}}]},{"cell_type":"code","source":"clean_memory() # 학습 전 garbaage collecting + cache 비우기","metadata":{"execution":{"iopub.status.busy":"2023-12-16T07:11:44.144475Z","iopub.execute_input":"2023-12-16T07:11:44.144832Z","iopub.status.idle":"2023-12-16T07:11:44.394884Z","shell.execute_reply.started":"2023-12-16T07:11:44.144804Z","shell.execute_reply":"2023-12-16T07:11:44.393737Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\n# models_dict = {}\n# scores = []\n\n# test_predict_list = []\n\n# best_params = {\n#     'boosting_type': 'gbdt', \n#     'metric': 'rmse',\n#     'reg_alpha': 0.003188447814669599, \n#     'reg_lambda': 0.0010228604507564066, \n#     'colsample_bytree': 0.5420247656839267, \n#     'subsample': 0.9778252382803456, \n#     'feature_fraction': 0.8,\n#     'bagging_freq': 1,\n#     'bagging_fraction': 0.75,\n#     'learning_rate': 0.01716485155812008, \n#     'num_leaves': 19, \n#     'min_child_samples': 46,\n#     'n_estimators': 12001,\n#     'verbosity': -1,\n#     'device_type': 'cpu'\n# }\n\n# # 시드 값 리스트\n# seed_values = [42, 52, 62, 72, 82]\n\n# # 각 시드 값에 대한 모델 학습 및 평가\n# for seed in seed_values:\n#     print(f\"Training with random state {seed}\")\n#     models_dict = {}\n#     scores = []\n\n#     # 타겟 변수를 구간으로 나누어 범주형 변수로 변환\n#     train_feats['target_cat'] = pd.cut(train_feats['score'], bins=13, labels=False)\n\n#     for i in range(5): \n#         skf = StratifiedKFold(n_splits=10, random_state=seed + i, shuffle=True)\n#         oof_valid_preds = np.zeros(train_feats.shape[0], )\n#         X_test = test_feats[train_cols]\n\n#         for fold, (train_idx, valid_idx) in enumerate(skf.split(train_feats, train_feats['target_cat'])):\n#             print(\"==-\"* 50)\n#             print(\"Fold : \", fold)\n\n#             X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n#             X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n#             print(\"Train :\", X_train.shape, y_train.shape)\n#             print(\"Valid :\", X_valid.shape, y_valid.shape)\n            \n#             params = {\n#             \"objective\": \"regression\",\n#             \"metric\": \"rmse\",\n#             'random_state': 42,\n#             \"n_estimators\" : 12001,\n#             \"verbosity\": -1,\n#             \"device_type\": \"cpu\",\n#             **best_params }\n            \n#             # 모델 생성 및 학습\n#             model = lgb.LGBMRegressor(**best_params)\n#             early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n#             verbose_callback = lgb.callback.record_evaluation({})\n\n#             model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n#                       callbacks=[early_stopping_callback, verbose_callback])\n\n#             valid_predict = model.predict(X_valid)\n#             oof_valid_preds[valid_idx] = valid_predict\n\n#             test_predict = model.predict(X_test)\n#             test_predict_list.append(test_predict)\n\n#             score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n#             print(\"Fold RMSE Score : \", score)\n\n#             models_dict[f'{fold}_{i}'] = model\n\n#         oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n#         scores.append(oof_score)\n#         print(\"OOF RMSE Score : \", oof_score)\n\n#     # 시드별 평균 OOF RMSE Score 계산\n#     avg_oof_score = np.mean(scores)\n#     print(f\"Average OOF RMSE Score for seed {seed}: {avg_oof_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T08:21:19.994078Z","iopub.execute_input":"2023-12-16T08:21:19.994470Z","iopub.status.idle":"2023-12-16T08:21:20.002823Z","shell.execute_reply.started":"2023-12-16T08:21:19.994440Z","shell.execute_reply":"2023-12-16T08:21:20.001411Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nmodels_dict = {}\nscores = []\n\ntest_predict_list = []\n\nbest_params = {\n    'boosting_type': 'gbdt', \n    'metric': 'rmse',\n    'reg_alpha': 0.003188447814669599, \n    'reg_lambda': 0.0010228604507564066, \n    'colsample_bytree': 0.5420247656839267, \n    'subsample': 0.9778252382803456, \n    'feature_fraction': 0.8,\n    'bagging_freq': 1,\n    'bagging_fraction': 0.75,\n    'learning_rate': 0.01716485155812008, \n    'num_leaves': 19, \n    'min_child_samples': 46,\n    'n_estimators': 12001,\n    'verbosity': -1,\n    'device_type': 'cpu'\n}\n\n\nseed_values = [72]\n\n# 각 시드 값에 대한 모델 학습 및 평가\nfor seed in seed_values:\n    print(f\"Training with random state {seed}\")\n    models_dict = {}\n    scores = []\n\n    # 타겟 변수를 구간으로 나누어 범주형 변수로 변환\n    train_feats['target_cat'] = pd.cut(train_feats['score'], bins=13, labels=False)\n\n    for i in range(5): \n        skf = StratifiedKFold(n_splits=10, random_state=seed + i, shuffle=True)\n        oof_valid_preds = np.zeros(train_feats.shape[0], )\n        X_test = test_feats[train_cols]\n\n        for fold, (train_idx, valid_idx) in enumerate(skf.split(train_feats, train_feats['target_cat'])):\n            print(\"==-\"* 50)\n            print(\"Fold : \", fold)\n\n            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n            print(\"Train :\", X_train.shape, y_train.shape)\n            print(\"Valid :\", X_valid.shape, y_valid.shape)\n            \n            params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            'random_state': 42,\n            \"n_estimators\" : 12001,\n            \"verbosity\": -1,\n            \"device_type\": \"cpu\",\n            **best_params }\n            \n            # 모델 생성 및 학습\n            model = lgb.LGBMRegressor(**best_params)\n            early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n            verbose_callback = lgb.callback.record_evaluation({})\n\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n                      callbacks=[early_stopping_callback, verbose_callback])\n\n            valid_predict = model.predict(X_valid)\n            oof_valid_preds[valid_idx] = valid_predict\n\n            test_predict = model.predict(X_test)\n            test_predict_list.append(test_predict)\n\n            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n            print(\"Fold RMSE Score : \", score)\n\n            models_dict[f'{fold}_{i}'] = model\n\n        oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n        scores.append(oof_score)\n        print(\"OOF RMSE Score : \", oof_score)\n\n    # 시드별 평균 OOF RMSE Score 계산\n    avg_oof_score = np.mean(scores)\n    print(f\"Average OOF RMSE Score for seed {seed}: {avg_oof_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:00:07.985016Z","iopub.execute_input":"2023-12-16T09:00:07.985418Z","iopub.status.idle":"2023-12-16T09:12:26.733532Z","shell.execute_reply.started":"2023-12-16T09:00:07.985386Z","shell.execute_reply":"2023-12-16T09:12:26.732024Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Training with random state 72\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrain : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5938188004806548\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5870193077091477\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6300958767286328\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6214949690898353\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6533469313843604\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5849104015655345\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6111428915501862\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5845088753487819\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6135241446211491\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5993942301217949\nOOF RMSE Score :  0.6082953854339709\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrain : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.612370926373456\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6285439732737933\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5918686579816661\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5993557767125703\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6416997945551136\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.615031468350576\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5752681337683071\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6149045584534367\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.584980160588904\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.615904514873243\nOOF RMSE Score :  0.6082964083567828\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrain : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5802613907076347\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6065040481567373\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6157942722100381\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5656111388417687\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.587616505623303\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6564972971780136\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6122825155612779\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6548117966337251\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5560798737867099\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.670042371416635\nOOF RMSE Score :  0.6116902119366703\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrain : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.587442304012666\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6228170091508984\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6416754143779118\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5865304776583733\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6315682392479612\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6197866925094709\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5961133835539784\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6049145353188129\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6260363914777554\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5619060451213423\nOOF RMSE Score :  0.6083256068645901\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrain : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6428325164865996\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6112271155653313\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6263025035594038\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6199921394280885\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6361337354701839\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6126054816083547\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.551828694263293\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6030873943210852\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.5807285460370989\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrain : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\nFold RMSE Score :  0.6015618517107801\nOOF RMSE Score :  0.6091757657218774\nAverage OOF RMSE Score for seed 72: 0.6091566756627783\n","output_type":"stream"}]},{"cell_type":"code","source":"# 모든 폴드에 대한 oof_score 평균  \n# stratified 적용하니 살짝 낮아짐. 근데 public은 오름 ㅅㅂ \n# stratified 42.52.72.82 다 적용해봤을 때의 평균 0.6110665452601025\n# stratified n_estimator 12001 , 72 :0.6091566756627783 -> 그래도 그냥 이걸로 하고 앙상블시 가중치 6으로 증가시켜\n\n\n# 그냥 k fold n_estimator 12001, seed 42: 0.6099770226513618\n# 그냥 Kfold n_estimator 12001, seed 72:  0.6099835885582975","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:12:26.735840Z","iopub.execute_input":"2023-12-16T09:12:26.736213Z","iopub.status.idle":"2023-12-16T09:12:26.741138Z","shell.execute_reply.started":"2023-12-16T09:12:26.736182Z","shell.execute_reply":"2023-12-16T09:12:26.740166Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"test_feats['score'] = np.mean(test_predict_list, axis=0)\npubliclgbm_pred = test_feats[['id', 'score']]\npubliclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:12:26.742246Z","iopub.execute_input":"2023-12-16T09:12:26.743077Z","iopub.status.idle":"2023-12-16T09:12:26.761320Z","shell.execute_reply.started":"2023-12-16T09:12:26.743045Z","shell.execute_reply":"2023-12-16T09:12:26.760094Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.455448\n1  2222bbbb  1.429760\n2  4444cccc  1.432950","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.455448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.429760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.432950</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_feats['score'] = np.mean(test_predict_list, axis=0)\npubliclgbm_pred = test_feats[['id', 'score']]\npubliclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:12:26.763531Z","iopub.execute_input":"2023-12-16T09:12:26.764014Z","iopub.status.idle":"2023-12-16T09:12:26.779091Z","shell.execute_reply.started":"2023-12-16T09:12:26.763980Z","shell.execute_reply":"2023-12-16T09:12:26.777733Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.455448\n1  2222bbbb  1.429760\n2  4444cccc  1.432950","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.455448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.429760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.432950</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 165 features lgbm + xgb ensemble","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\nactivities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\nevents = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\ntext_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n\n# unique value 갯수 계산\ndef count_by_values(df, colname, values):\n    fts = df.select(pl.col('id').unique(maintain_order=True))\n    for i, value in enumerate(values):\n        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n        fts  = fts.join(tmp_df, on='id', how='left') \n    return fts\n\n\ndef dev_feats(df):\n    # unique value 갯수 계산\n    print(\"< Count by values features >\")\n    \n    feats = count_by_values(df, 'activity', activities)\n    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n    # 단어의 통계량 계산\n    print(\"< Input words stats features >\")\n\n    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n    temp = temp.drop('text_change')\n    feats = feats.join(temp, on='id', how='left') \n\n\n    # 합계, 평균, 표준편차, 중앙값, 최소값, 최대값, 분위수를 계산\n    print(\"< Numerical columns features >\")\n\n    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n    feats = feats.join(temp, on='id', how='left') \n\n    # 범주형 열에 대해 고유값의 개수를 집계\n    print(\"< Categorical columns features >\")\n    \n    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    # 사용자의 입력 사이의 시간 차이(time_diff)를 계산하여, 특정 시간 간격 내의 휴식 횟수와 관련 통계를 집계\n    print(\"< Idle time features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n                                   inter_key_median_lantency = pl.median('time_diff'),\n                                   mean_pause_time = pl.mean('time_diff'),\n                                   std_pause_time = pl.std('time_diff'),\n                                   total_pause_time = pl.sum('time_diff'),\n                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n    feats = feats.join(temp, on='id', how='left') \n    # 'P-bursts'는 특정 조건(예: 시간 차이가 2초 미만)을 만족하는 연속된 이벤트의 수를 집계\n    print(\"< P-bursts features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('time_diff')<2)\n    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left') \n\n    #'R-bursts'는 'Remove/Cut' 같은 특정 활동이 연속으로 발생하는 횟수를 집계\n    print(\"< R-bursts features >\")\n\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left')\n    \n    return feats\n\n\ndef train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x.iloc[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x.iloc[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\ndef evaluate(data_x, data_y, model, random_state=42, n_splits=10, test_x=None):\n    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n        model.fit(train_x, train_y)\n        if test_x is None:\n            test_y[valid_index] = model.predict(valid_x)\n        else:\n            test_y[:, i] = model.predict(test_x)\n    return test_y if (test_x is None) else np.mean(test_y, axis=1)\n\n# def evaluate(data_x, data_y, model, random_state=42, n_splits=10, test_x=None):\n#     skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n#     test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n#     fold_scores = []  # 폴드별 점수를 저장할 리스트\n\n#     for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n#         train_x, train_y = data_x[train_index], data_y[train_index]\n#         valid_x, valid_y = data_x[valid_index], data_y[valid_index]\n\n#         model.fit(train_x, train_y)\n#         if test_x is None:\n#             valid_predictions = model.predict(valid_x)\n#             test_y[valid_index] = valid_predictions\n#             fold_score = mean_squared_error(valid_y, valid_predictions, squared=False)\n#             fold_scores.append(fold_score)\n#             print(f\"Fold {i} RMSE Score: {fold_score}\")\n#         else:\n#             test_y[:, i] = model.predict(test_x)\n\n#     if test_x is None:\n#         average_score = np.mean(fold_scores)\n#         print(f\"Average CV RMSE Score: {average_score}\")\n#         return test_y\n#     else:\n#         return np.mean(test_y, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:16:34.402737Z","iopub.execute_input":"2023-12-16T09:16:34.403112Z","iopub.status.idle":"2023-12-16T09:16:34.438522Z","shell.execute_reply.started":"2023-12-16T09:16:34.403084Z","shell.execute_reply":"2023-12-16T09:16:34.436920Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\ndef reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\n\ndef get_essay_df(df):\n    df       = df[df.activity != 'Nonproduction']\n    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n    return essay_df\n\n\ndef word_feats(df):\n    essay_df = df\n    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('word')\n    df['word_len'] = df['word'].apply(lambda x: len(x))\n    df = df[df['word_len'] != 0]\n\n    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n    word_agg_df['id'] = word_agg_df.index\n    word_agg_df = word_agg_df.reset_index(drop=True)\n    return word_agg_df\n\n\ndef sent_feats(df):\n    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('sent')\n    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n    df = df[df.sent_len!=0].reset_index(drop=True)\n\n    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n\ndef parag_feats(df):\n    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n    df = df.explode('paragraph')\n    # Number of characters in paragraphs\n    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n    df = df[df.paragraph_len!=0].reset_index(drop=True)\n    \n    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n\ndef product_to_keys(logs, essays):\n    essays['product_len'] = essays.essay.str.len()\n    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n    essays = essays.merge(tmp_df, on='id', how='left')\n    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n    return essays[['id', 'product_to_keys']]\n\ndef get_keys_pressed_per_second(logs):\n    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n    return temp_df[['id', 'keys_per_second']]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:16:34.588339Z","iopub.execute_input":"2023-12-16T09:16:34.588888Z","iopub.status.idle":"2023-12-16T09:16:34.623421Z","shell.execute_reply.started":"2023-12-16T09:16:34.588847Z","shell.execute_reply":"2023-12-16T09:16:34.621887Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\ntrain_logs    = pl.scan_csv(data_path + 'train_logs.csv')\ntrain_feats   = dev_feats(train_logs)\ntrain_feats   = train_feats.collect().to_pandas()\n\nprint('< Essay Reconstruction >')\ntrain_logs             = train_logs.collect().to_pandas()\ntrain_essays           = get_essay_df(train_logs)\ntrain_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\ntrain_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n\n\nprint('< Mapping >')\ntrain_scores   = pd.read_csv(data_path + 'train_scores.csv')\ndata           = train_feats.merge(train_scores, on='id', how='left')\nx              = data.drop(['id', 'score'], axis=1)\ny              = data['score'].values\nprint(f'Number of features: {len(x.columns)}')\n\n\nprint('< Testing Data >')\ntest_logs   = pl.scan_csv(data_path + 'test_logs.csv')\ntest_feats  = dev_feats(test_logs)\ntest_feats  = test_feats.collect().to_pandas()\n\ntest_logs             = test_logs.collect().to_pandas()\ntest_essays           = get_essay_df(test_logs)\ntest_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\ntest_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n\n\ntest_ids = test_feats['id'].values\ntestin_x = test_feats.drop(['id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:16:34.626106Z","iopub.execute_input":"2023-12-16T09:16:34.626523Z","iopub.status.idle":"2023-12-16T09:17:49.551429Z","shell.execute_reply.started":"2023-12-16T09:16:34.626485Z","shell.execute_reply":"2023-12-16T09:17:49.550403Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n< Essay Reconstruction >\n< Mapping >\nNumber of features: 165\n< Testing Data >\n< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n","output_type":"stream"}]},{"cell_type":"code","source":"print('< LGBM Learning and Evaluation >')\nlgbm_param = {'n_estimators': 1024,\n         'learning_rate': 0.005,\n         'metric': 'rmse',\n         'random_state': 42,\n         'force_col_wise': True,\n         'verbosity': 0,}\nlgbm_solution = LGBMRegressor(**lgbm_param)\nlgbm_pred   = evaluate(x.copy(), y.copy(), lgbm_solution, test_x=testin_x.copy()) \n\nlgbm_sub = pd.DataFrame({'id': test_ids, 'score': lgbm_pred})\n#sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:17:49.552411Z","iopub.execute_input":"2023-12-16T09:17:49.553714Z","iopub.status.idle":"2023-12-16T09:20:53.238382Z","shell.execute_reply.started":"2023-12-16T09:17:49.553650Z","shell.execute_reply":"2023-12-16T09:20:53.236688Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"< LGBM Learning and Evaluation >\n","output_type":"stream"}]},{"cell_type":"code","source":"lgbm_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:20:53.241372Z","iopub.execute_input":"2023-12-16T09:20:53.241714Z","iopub.status.idle":"2023-12-16T09:20:53.252168Z","shell.execute_reply.started":"2023-12-16T09:20:53.241688Z","shell.execute_reply":"2023-12-16T09:20:53.250708Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.280492\n1  2222bbbb  1.275996\n2  4444cccc  1.276752","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.280492</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.275996</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.276752</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('< XGB Learning and Evaluation >')\n\nxgb_param={\n'reg_alpha': 0.0008774661176012108,\n'reg_lambda': 2.542812743920178,\n'colsample_bynode': 0.7839026197349153,\n'subsample': 0.8994226268096415,\n'eta': 0.04730766698056879, \n'max_depth': 3, \n'n_estimators': 1024,\n'random_state': 42,\n'eval_metric': 'rmse'\n}\n\nxgb_solution = XGBRegressor(**xgb_param)\nxgb_pred   = evaluate(x.copy(), y.copy(), xgb_solution, test_x=testin_x.copy()) \n\nxgb_sub = pd.DataFrame({'id': test_ids, 'score': xgb_pred})\n#xgb_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:20:53.254086Z","iopub.execute_input":"2023-12-16T09:20:53.254470Z","iopub.status.idle":"2023-12-16T09:21:40.026290Z","shell.execute_reply.started":"2023-12-16T09:20:53.254437Z","shell.execute_reply":"2023-12-16T09:21:40.024392Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"< XGB Learning and Evaluation >\n","output_type":"stream"}]},{"cell_type":"code","source":"xgb_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:21:40.031486Z","iopub.execute_input":"2023-12-16T09:21:40.031876Z","iopub.status.idle":"2023-12-16T09:21:40.042942Z","shell.execute_reply.started":"2023-12-16T09:21:40.031847Z","shell.execute_reply":"2023-12-16T09:21:40.041527Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  2.754787\n1  2222bbbb  1.358807\n2  4444cccc  1.376992","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2.754787</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.358807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.376992</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"best_w =0.81 \n\nW = [best_w, 1 - best_w]\nprint(W)\nensemble_preds = lgbm_pred * W[0] + xgb_pred * W[1]\nensemble_preds","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:21:40.044453Z","iopub.execute_input":"2023-12-16T09:21:40.044806Z","iopub.status.idle":"2023-12-16T09:21:40.064724Z","shell.execute_reply.started":"2023-12-16T09:21:40.044776Z","shell.execute_reply":"2023-12-16T09:21:40.063465Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"[0.81, 0.18999999999999995]\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"array([1.56060808, 1.29172988, 1.29579737])"},"metadata":{}}]},{"cell_type":"code","source":"ensemble_sub = pd.DataFrame({'id': test_ids, 'score': ensemble_preds})\nensemble_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:21:40.066431Z","iopub.execute_input":"2023-12-16T09:21:40.066826Z","iopub.status.idle":"2023-12-16T09:21:40.081790Z","shell.execute_reply.started":"2023-12-16T09:21:40.066792Z","shell.execute_reply":"2023-12-16T09:21:40.080631Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.560608\n1  2222bbbb  1.291730\n2  4444cccc  1.295797","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.560608</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.291730</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.295797</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# public lgbm + ensemble_sub 최종 merge \n\n# 두 데이터프레임을 'id' 컬럼을 기준으로 병합\nmerged_df = ensemble_sub.merge(publiclgbm_pred, on='id', suffixes=('_ensemble', '_publiclgbm'))\n\n# 가중치를 반반\nweight_ensemble = 0.4\nweight_publiclgbm = 0.6\n\n# 가중 평균 점수 계산하여 원래의 'score' 컬럼에 저장\nmerged_df['score'] = (merged_df['score_ensemble'] * weight_ensemble) + (merged_df['score_publiclgbm'] * weight_publiclgbm)\nmerged_df.drop(['score_ensemble', 'score_publiclgbm'], axis=1, inplace=True)\n\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:21:40.086385Z","iopub.execute_input":"2023-12-16T09:21:40.086760Z","iopub.status.idle":"2023-12-16T09:21:40.108084Z","shell.execute_reply.started":"2023-12-16T09:21:40.086729Z","shell.execute_reply":"2023-12-16T09:21:40.106036Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.497512\n1  2222bbbb  1.374548\n2  4444cccc  1.378089","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.497512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.374548</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.378089</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T09:21:40.111015Z","iopub.execute_input":"2023-12-16T09:21:40.111371Z","iopub.status.idle":"2023-12-16T09:21:40.121986Z","shell.execute_reply.started":"2023-12-16T09:21:40.111339Z","shell.execute_reply":"2023-12-16T09:21:40.120882Z"},"trusted":true},"execution_count":52,"outputs":[]}]}
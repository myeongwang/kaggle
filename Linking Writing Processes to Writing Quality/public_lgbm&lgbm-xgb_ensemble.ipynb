{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"}],"dockerImageVersionId":30616,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. count_by_values(df, colname, values)\n\n# 이 함수는 DataFrame의 특정 열에서 특정 값들의 발생 횟수를 계산\n# 고유한 'id' 값들을 가진 DataFrame fts를 초기화합니다\n# values에 있는 각 값에 대해, 지정된 colname에서 그 값의 발생 횟수를 계산하는 임시 DataFrame tmp_df를 생성합니다.\n# tmp_df를 'id'를 기준으로 fts와 결합하고, values에 있는 모든 값에 대해 이 과정을 반복합니다.\n# 함수는 각 고유 'id'에 대해 지정된 값들의 개수를 포함하는 fts를 반환합니다.\n\n# 2. dev_feats(df)\n# 이 함수는 DataFrame df에서 특성을 개발하기 위해 다양한 계산과 데이터 변환을 수행\n\n# count_by_values 함수를 사용하여 'activity', 'text_change', 'down_event', 'up_event' 열의 값에 대한 특성을 생성\n# 텍스트 변경에 관련된 통계적 특성을 계산합니다. 예를 들어, 입력 단어 수, 최대/평균/표준편차 길이 등을 계산\n# 'action_time' 및 다른 숫자형 열에 대한 요약 통계(합계, 평균, 표준편차 등)를 계산\n# 카테고리형 열에 대한 특성을 생성합니다. 예를 들어, 고유한 'activity', 'down_event', 'up_event', 'text_change' 값의 수를 계산\n# 유휴 시간에 대한 특성을 계산합니다. 예를 들어, 키 입력 간의 최대 지연 시간, 중간 지연 시간 등을 계산\n# 'P-bursts'와 'R-bursts' 특성을 계산합니다. 이는 특정 행동 패턴(예: 연속적인 입력 또는 삭제)에 대한 통계를 나타냅니다.","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:36.704985Z","iopub.execute_input":"2023-12-09T05:09:36.705793Z","iopub.status.idle":"2023-12-09T05:09:36.711885Z","shell.execute_reply.started":"2023-12-09T05:09:36.705755Z","shell.execute_reply":"2023-12-09T05:09:36.710634Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# 1. q1(x) & q3(x)\n\n# q1(x)는 주어진 데이터의 하위 25% (제1사분위수)를 계산\n# q3(x)는 주어진 데이터의 상위 25% (제3사분위수)를 계산\n\n# 2. reconstruct_essay(currTextInput)\n\n# 이 함수는 주어진 입력 로그를 바탕으로 에세이 텍스트를 재구성\n# 'Replace', 'Paste', 'Remove/Cut', 'Move' 등의 행동을 확인하고, 이에 따라 텍스트를 수정합니다.\n# 예를 들어, 'Replace' 동작은 특정 부분의 텍스트를 다른 텍스트로 교체합니다.\n\n# 3. get_essay_df(df)\n\n# 이 함수는 입력된 DataFrame을 사용하여 각 'id'에 대한 에세이 텍스트를 재구성\n# 'Nonproduction' 활동을 제외하고, 나머지 활동으로부터 에세이를 재구성\n# 재구성된 에세이를 'id'와 함께 새로운 DataFrame으로 반환\n\n# 4. word_feats(df)\n\n# 이 함수는 에세이 텍스트에서 단어 관련 특성을 추출\n# 단어 길이, 단어 수 등을 계산하고, 이러한 값들에 대한 다양한 통계치를 계산합니다(예: 평균, 최소값, 최대값 등).\n\n# 5. sent_feats(df)\n\n# 이 함수는 에세이 텍스트에서 문장 관련 특성을 추출\n# 문장 길이, 문장 내 단어 수 등을 계산하고, 이러한 값들에 대한 다양한 통계치를 계산\n\n# 6. parag_feats(df)\n\n# 이 함수는 에세이 텍스트에서 문단 관련 특성을 추출\n# 문단 길이, 문단 내 단어 수 등을 계산하고, 이러한 값들에 대한 다양한 통계치를 계산\n\n# 7. product_to_keys(logs, essays)\n\n# 이 함수는 입력된 로그와 에세이를 바탕으로 'product_len'과 'keys_pressed' 비율을 계산\n# 'product_len'은 에세이의 길이, 'keys_pressed'는 입력 및 삭제된 키의 수\n# 최종적으로 'product_to_keys' 비율을 반환\n\n# 8. get_keys_pressed_per_second(logs)\n\n# 이 함수는 입력 및 삭제된 키의 수를 시간 단위로 계산\n# 'keys_pressed'는 입력 및 삭제된 키의 수, 'min_down_time'과 'max_up_time'은 각각 최소 다운 타임과 최대 업 타임\n# 이를 바탕으로 초당 키 입력 수 ('keys_per_second')를 계산","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:36.715528Z","iopub.execute_input":"2023-12-09T05:09:36.716125Z","iopub.status.idle":"2023-12-09T05:09:36.725642Z","shell.execute_reply.started":"2023-12-09T05:09:36.716100Z","shell.execute_reply":"2023-12-09T05:09:36.724778Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"## lgbm,xgb blending 시의 최적의 가중치 구해보자 \n\n# def train_valid_split(data_x, data_y, train_idx, valid_idx):\n#     x_train = data_x.iloc[train_idx]\n#     y_train = data_y[train_idx]\n#     x_valid = data_x.iloc[valid_idx]\n#     y_valid = data_y[valid_idx]\n#     return x_train, y_train, x_valid, y_valid\n\n# # LightGBM \n# lgbm_param = {\n#     'n_estimators': 1024,\n#     'learning_rate': 0.005,\n#     'metric': 'rmse',\n#     'random_state': 42\n# }\n\n# # XGB\n# xgb_param = {\n#     'reg_alpha': 0.0008774661176012108,\n#     'reg_lambda': 2.542812743920178,\n#     'colsample_bynode': 0.7839026197349153,\n#     'subsample': 0.8994226268096415,\n#     'eta': 0.04730766698056879, \n#     'max_depth': 3, \n#     'n_estimators': 1024,\n#     'random_state': 42,\n#     'eval_metric': 'rmse'\n# }\n\n# lgbm_model = LGBMRegressor(**lgbm_param)\n# xgb_model = XGBRegressor(**xgb_param)\n\n# # stratified kfold 적용\n# skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n# lgbm_preds = []\n# xgb_preds = []\n# valid_y_list = []\n\n# for train_index, valid_index in skf.split(x, y.astype(str)):     \n#     train_x, train_y, valid_x, valid_y = train_valid_split(x, y, train_index, valid_index)\n\n#     lgbm_model.fit(train_x, train_y)\n#     lgbm_preds.append(lgbm_model.predict(valid_x))\n\n#     xgb_model.fit(train_x, train_y)\n#     xgb_preds.append(xgb_model.predict(valid_x))\n\n#     # 검증 타겟 저장\n#     valid_y_list.append(valid_y)\n\n# # 모든 fold의 예측값과 실제값 통합\n# lgbm_preds = np.concatenate(lgbm_preds)\n# xgb_preds = np.concatenate(xgb_preds)\n# valid_y_combined = np.concatenate(valid_y_list)\n\n# # 최적 가중치 \n# best_sc = float('inf')\n# best_w = 0\n# for w in np.arange(0, 1.01, 0.01):\n#     combined_pred = w * lgbm_preds + (1-w) * xgb_preds\n#     sc = mean_squared_error(valid_y_combined, combined_pred, squared=False)\n#     if sc < best_sc:\n#         best_sc = sc\n#         best_w = w\n\n# print('최적의 RMSE 점수 = {:.5f}'.format(best_sc))\n# print('최적의 가중치 W = {:.3f}'.format(best_w))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:36.730315Z","iopub.execute_input":"2023-12-09T05:09:36.730618Z","iopub.status.idle":"2023-12-09T05:09:36.739148Z","shell.execute_reply.started":"2023-12-09T05:09:36.730594Z","shell.execute_reply":"2023-12-09T05:09:36.738398Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# public lgbm 선 적용 이후 lgbm+xgb랑 가중치 조절 후 앙상블 한 번 더 ㄱ ","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:36.740542Z","iopub.execute_input":"2023-12-09T05:09:36.740838Z","iopub.status.idle":"2023-12-09T05:09:36.751658Z","shell.execute_reply.started":"2023-12-09T05:09:36.740815Z","shell.execute_reply":"2023-12-09T05:09:36.750738Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport ctypes\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:36.753215Z","iopub.execute_input":"2023-12-09T05:09:36.753488Z","iopub.status.idle":"2023-12-09T05:09:38.557570Z","shell.execute_reply.started":"2023-12-09T05:09:36.753465Z","shell.execute_reply":"2023-12-09T05:09:38.556775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nimport gc\nimport os\nimport itertools\nimport pickle\n\nfrom random import choice, choices\nfrom functools import reduce\nfrom tqdm import tqdm\nfrom itertools import cycle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom functools import reduce\nfrom itertools import cycle\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nfrom sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\nfrom transformers import BertTokenizer\nimport warnings\n\nimport os\nimport gc\nimport re\nimport random\nfrom collections import Counter, defaultdict\nimport pprint\nimport time\nimport copy\nimport lightgbm as lgb\n\nimport seaborn as sns\nfrom tqdm.autonotebook import tqdm\n\n# from gensim.models import Word2Vec\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:38.558720Z","iopub.execute_input":"2023-12-09T05:09:38.559083Z","iopub.status.idle":"2023-12-09T05:09:41.806838Z","shell.execute_reply.started":"2023-12-09T05:09:38.559056Z","shell.execute_reply":"2023-12-09T05:09:41.805832Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def getEssays(df):\n    # Copy required columns\n    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    \n    # Get rid of text inputs that make no change\n    # Note: Shift was unpreditcable so ignored\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n\n    # Get how much each Id there is\n    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n\n    # Holds the final index of the previous Id\n    lastIndex = 0\n\n    # Holds all the essays\n    essaySeries = pd.Series()\n\n    # Fills essay series with essays\n    for index, valCount in enumerate(valCountsArr):\n\n        # Indexes down_time at current Id\n        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n\n        # Update the last index\n        lastIndex += valCount\n\n        # Where the essay content will be stored\n        essayText = \"\"\n\n        \n        # Produces the essay\n        for Input in currTextInput.values:\n            \n            # Input[0] = activity\n            # Input[2] = cursor_position\n            # Input[3] = text_change\n            \n            # If activity = Replace\n            if Input[0] == 'Replace':\n                # splits text_change at ' => '\n                replaceTxt = Input[2].split(' => ')\n                \n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n\n                \n            # If activity = Paste    \n            if Input[0] == 'Paste':\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n\n                \n            # If activity = Remove/Cut\n            if Input[0] == 'Remove/Cut':\n                # DONT TOUCH\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n\n                \n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # Gets rid of the \"Move from to\" text\n                croppedTxt = Input[0][10:]\n                \n                # Splits cropped text by ' To '\n                splitTxt = croppedTxt.split(' To ')\n                \n                # Splits split text again by ', ' for each item\n                valueArr = [item.split(', ') for item in splitTxt]\n                \n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n\n                # Skip if someone manages to activiate this by moving to same place\n                if moveData[0] != moveData[2]:\n                    # Check if they move text forward in essay (they are different)\n                    if moveData[0] < moveData[2]:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    else:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue\n                \n                \n            # If just input\n            # DONT TOUCH\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n\n            \n        # Sets essay at index  \n        essaySeries[index] = essayText\n     \n    \n    # Sets essay series index to the ids\n    essaySeries.index =  textInputDf['id'].unique()\n    \n    \n    # Returns the essay series\n    return essaySeries","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:41.808952Z","iopub.execute_input":"2023-12-09T05:09:41.809244Z","iopub.status.idle":"2023-12-09T05:09:41.826254Z","shell.execute_reply.started":"2023-12-09T05:09:41.809218Z","shell.execute_reply":"2023-12-09T05:09:41.825273Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\ntrain_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\ntestdf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:41.827477Z","iopub.execute_input":"2023-12-09T05:09:41.827716Z","iopub.status.idle":"2023-12-09T05:09:53.615834Z","shell.execute_reply.started":"2023-12-09T05:09:41.827694Z","shell.execute_reply":"2023-12-09T05:09:53.614963Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_essays = getEssays(traindf)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:09:53.616928Z","iopub.execute_input":"2023-12-09T05:09:53.617213Z","iopub.status.idle":"2023-12-09T05:20:04.004192Z","shell.execute_reply.started":"2023-12-09T05:09:53.617188Z","shell.execute_reply":"2023-12-09T05:20:04.003171Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 7min 8s, sys: 3min 1s, total: 10min 10s\nWall time: 10min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_essays = getEssays(testdf)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:04.005904Z","iopub.execute_input":"2023-12-09T05:20:04.006306Z","iopub.status.idle":"2023-12-09T05:20:04.016827Z","shell.execute_reply.started":"2023-12-09T05:20:04.006269Z","shell.execute_reply":"2023-12-09T05:20:04.015937Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\ntest_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:04.018187Z","iopub.execute_input":"2023-12-09T05:20:04.018965Z","iopub.status.idle":"2023-12-09T05:20:04.029581Z","shell.execute_reply.started":"2023-12-09T05:20:04.018930Z","shell.execute_reply":"2023-12-09T05:20:04.028728Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"merged_data = train_essaysdf.merge(train_scores, on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:04.033893Z","iopub.execute_input":"2023-12-09T05:20:04.034138Z","iopub.status.idle":"2023-12-09T05:20:04.043428Z","shell.execute_reply.started":"2023-12-09T05:20:04.034117Z","shell.execute_reply":"2023-12-09T05:20:04.042642Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(ngram_range=(1, 2))\nX_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\nX_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\ncount_vectorizer.get_feature_names_out() #ADDED\ny = merged_data['score']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:04.044545Z","iopub.execute_input":"2023-12-09T05:20:04.044915Z","iopub.status.idle":"2023-12-09T05:20:05.447690Z","shell.execute_reply.started":"2023-12-09T05:20:04.044865Z","shell.execute_reply":"2023-12-09T05:20:05.446819Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame()\ndf_test = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:05.448810Z","iopub.execute_input":"2023-12-09T05:20:05.449135Z","iopub.status.idle":"2023-12-09T05:20:05.455674Z","shell.execute_reply.started":"2023-12-09T05:20:05.449106Z","shell.execute_reply":"2023-12-09T05:20:05.454746Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_tokenizer_train = X_tokenizer_train.todense()\nX_tokenizer_test = X_tokenizer_test.todense()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:05.456903Z","iopub.execute_input":"2023-12-09T05:20:05.457258Z","iopub.status.idle":"2023-12-09T05:20:05.470487Z","shell.execute_reply.started":"2023-12-09T05:20:05.457230Z","shell.execute_reply":"2023-12-09T05:20:05.469530Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for i in range(X_tokenizer_train.shape[1]) : \n    L = list(X_tokenizer_train[:,i])\n    li = [int(x) for x in L ]\n    df_train[f'feature {i}'] = li","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:05.471536Z","iopub.execute_input":"2023-12-09T05:20:05.472572Z","iopub.status.idle":"2023-12-09T05:20:09.734665Z","shell.execute_reply.started":"2023-12-09T05:20:05.472547Z","shell.execute_reply":"2023-12-09T05:20:09.733416Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for i in range(X_tokenizer_test.shape[1]) : \n    L = list(X_tokenizer_test[:,i])\n    li = [int(x) for x in L ]\n    df_test[f'feature {i}'] = li","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:09.735839Z","iopub.execute_input":"2023-12-09T05:20:09.736874Z","iopub.status.idle":"2023-12-09T05:20:09.857013Z","shell.execute_reply.started":"2023-12-09T05:20:09.736844Z","shell.execute_reply":"2023-12-09T05:20:09.855976Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_train_index = train_essaysdf['id']\ndf_test_index = test_essaysdf['id']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:09.858410Z","iopub.execute_input":"2023-12-09T05:20:09.858722Z","iopub.status.idle":"2023-12-09T05:20:09.863326Z","shell.execute_reply.started":"2023-12-09T05:20:09.858694Z","shell.execute_reply":"2023-12-09T05:20:09.862407Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train.loc[:, 'id'] = df_train_index\ndf_test.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:09.864516Z","iopub.execute_input":"2023-12-09T05:20:09.864804Z","iopub.status.idle":"2023-12-09T05:20:09.874716Z","shell.execute_reply.started":"2023-12-09T05:20:09.864779Z","shell.execute_reply":"2023-12-09T05:20:09.873851Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_agg_fe_df = traindf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:09.875758Z","iopub.execute_input":"2023-12-09T05:20:09.876027Z","iopub.status.idle":"2023-12-09T05:20:14.657023Z","shell.execute_reply.started":"2023-12-09T05:20:09.876003Z","shell.execute_reply":"2023-12-09T05:20:14.656239Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_agg_fe_df = testdf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:14.658045Z","iopub.execute_input":"2023-12-09T05:20:14.658302Z","iopub.status.idle":"2023-12-09T05:20:14.677887Z","shell.execute_reply.started":"2023-12-09T05:20:14.658279Z","shell.execute_reply":"2023-12-09T05:20:14.677147Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nclass Preprocessor:\n    \n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n        \n        self.idf = defaultdict(float)\n#         self.gaps = [1, 2]\n    \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n\n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df):\n        \n        print(\"Starting to engineer features\")\n        \n        # initialize features dataframe\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        # get shifted features\n        # time shift\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # cursor position shift\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        # word count shift\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # get aggregate statistical features\n        print(\"Engineering statistical summaries for features\")\n        # [(feature name, [ stat summaries to add ])]\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew']),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew'])\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                    \n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        # counts\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        # input words\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n\n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n        \n        print(\"Done!\")\n        return feats","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:14.679200Z","iopub.execute_input":"2023-12-09T05:20:14.679905Z","iopub.status.idle":"2023-12-09T05:20:14.729087Z","shell.execute_reply.started":"2023-12-09T05:20:14.679871Z","shell.execute_reply":"2023-12-09T05:20:14.728268Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# 밑에서 kurt 관련 super() error 발생해서 statistics 중에서 걍 위에서 kurt 지워버렸음","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:14.729996Z","iopub.execute_input":"2023-12-09T05:20:14.730238Z","iopub.status.idle":"2023-12-09T05:20:14.742295Z","shell.execute_reply.started":"2023-12-09T05:20:14.730217Z","shell.execute_reply":"2023-12-09T05:20:14.741630Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"preprocessor = Preprocessor(seed=42)\n\nprint(\"Engineering features for training data\")\n\nother_train_feats = preprocessor.make_feats(traindf)\n\nprint()\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\nprint(\"-\"*25)\nother_test_feats = preprocessor.make_feats(testdf)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:20:14.743435Z","iopub.execute_input":"2023-12-09T05:20:14.743697Z","iopub.status.idle":"2023-12-09T05:25:03.999469Z","shell.execute_reply.started":"2023-12-09T05:20:14.743674Z","shell.execute_reply":"2023-12-09T05:25:03.998389Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Engineering features for training data\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f1e0aea9eb4c90a481b5a91685d6d2"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b29cc833b2d479b8c221769b1a4019e"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc055cacfc064e8ab3ef8553a09d94be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e63c860204b409d89d0b65c70159a4a"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66770bfdbc0746d8bf7e2918adfd2a5c"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbe2764d726450a999de3ae10503baa"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n\n-------------------------\nEngineering features for test data\n-------------------------\nStarting to engineer features\nEngineering time data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering cursor position data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering word count data\n> for gap 1\n> for gap 2\n> for gap 3\n> for gap 5\n> for gap 10\n> for gap 20\n> for gap 50\n> for gap 100\nEngineering statistical summaries for features\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99715c2a0e7440e882484dc53e48202e"}},"metadata":{}},{"name":"stdout","text":"Engineering activity counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73cc33a1bb874783b7ac8042aa962b55"}},"metadata":{}},{"name":"stdout","text":"Engineering event counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb436883df945dea1c566512db42b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"214aea6c0c474e7dbc22dc7262e1e5f8"}},"metadata":{}},{"name":"stdout","text":"Engineering text change counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46114580b3c435d98596d670b7020ff"}},"metadata":{}},{"name":"stdout","text":"Engineering punctuation counts data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd884f9dde047f995c43460a03f4519"}},"metadata":{}},{"name":"stdout","text":"Engineering input words data\nEngineering ratios data\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train_all = pd.DataFrame()\ndf_test_all = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.000978Z","iopub.execute_input":"2023-12-09T05:25:04.001439Z","iopub.status.idle":"2023-12-09T05:25:04.007620Z","shell.execute_reply.started":"2023-12-09T05:25:04.001400Z","shell.execute_reply":"2023-12-09T05:25:04.006428Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_train_all = df_train.merge(train_agg_fe_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.009324Z","iopub.execute_input":"2023-12-09T05:25:04.009708Z","iopub.status.idle":"2023-12-09T05:25:04.033187Z","shell.execute_reply.started":"2023-12-09T05:25:04.009671Z","shell.execute_reply":"2023-12-09T05:25:04.032228Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_test_all = df_test.merge(test_agg_fe_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.042467Z","iopub.execute_input":"2023-12-09T05:25:04.042805Z","iopub.status.idle":"2023-12-09T05:25:04.059493Z","shell.execute_reply.started":"2023-12-09T05:25:04.042772Z","shell.execute_reply":"2023-12-09T05:25:04.058554Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.061016Z","iopub.execute_input":"2023-12-09T05:25:04.061378Z","iopub.status.idle":"2023-12-09T05:25:04.066593Z","shell.execute_reply.started":"2023-12-09T05:25:04.061325Z","shell.execute_reply":"2023-12-09T05:25:04.065517Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', 'sum']\n\ndef split_essays_into_sentences(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n    essay_df = essay_df.explode('sent')\n    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n    return essay_df\n\ndef compute_sentence_aggregations(df):\n    sent_agg_df = pd.concat(\n        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    )\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\ndef split_essays_into_paragraphs(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n    essay_df = essay_df.explode('paragraph')\n    # Number of characters in paragraphs\n    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n    return essay_df\n\ndef compute_paragraph_aggregations(df):\n    paragraph_agg_df = pd.concat(\n        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    ) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.068428Z","iopub.execute_input":"2023-12-09T05:25:04.068752Z","iopub.status.idle":"2023-12-09T05:25:04.088435Z","shell.execute_reply.started":"2023-12-09T05:25:04.068720Z","shell.execute_reply":"2023-12-09T05:25:04.087388Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_sent_df = split_essays_into_sentences(train_essaysdf)\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:04.089905Z","iopub.execute_input":"2023-12-09T05:25:04.090244Z","iopub.status.idle":"2023-12-09T05:25:09.612327Z","shell.execute_reply.started":"2023-12-09T05:25:04.090212Z","shell.execute_reply":"2023-12-09T05:25:09.611566Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:09.613640Z","iopub.execute_input":"2023-12-09T05:25:09.614268Z","iopub.status.idle":"2023-12-09T05:25:15.220873Z","shell.execute_reply.started":"2023-12-09T05:25:09.614229Z","shell.execute_reply":"2023-12-09T05:25:15.220038Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.222011Z","iopub.execute_input":"2023-12-09T05:25:15.222341Z","iopub.status.idle":"2023-12-09T05:25:15.273026Z","shell.execute_reply.started":"2023-12-09T05:25:15.222299Z","shell.execute_reply":"2023-12-09T05:25:15.272254Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_paragraph_agg_df.loc[:, 'id'] = df_train_index\ntrain_sent_agg_df.loc[:, 'id'] = df_train_index","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.274028Z","iopub.execute_input":"2023-12-09T05:25:15.274322Z","iopub.status.idle":"2023-12-09T05:25:15.280492Z","shell.execute_reply.started":"2023-12-09T05:25:15.274285Z","shell.execute_reply":"2023-12-09T05:25:15.279486Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test_paragraph_agg_df.loc[:, 'id'] = df_test_index\ntest_sent_agg_df.loc[:, 'id'] = df_test_index","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.281754Z","iopub.execute_input":"2023-12-09T05:25:15.282020Z","iopub.status.idle":"2023-12-09T05:25:15.290401Z","shell.execute_reply.started":"2023-12-09T05:25:15.281996Z","shell.execute_reply":"2023-12-09T05:25:15.289574Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"new_train_feats = pd.DataFrame()\nnew_test_feats = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.291259Z","iopub.execute_input":"2023-12-09T05:25:15.291525Z","iopub.status.idle":"2023-12-09T05:25:15.300989Z","shell.execute_reply.started":"2023-12-09T05:25:15.291501Z","shell.execute_reply":"2023-12-09T05:25:15.300262Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"new_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\nnew_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.301923Z","iopub.execute_input":"2023-12-09T05:25:15.302151Z","iopub.status.idle":"2023-12-09T05:25:15.326567Z","shell.execute_reply.started":"2023-12-09T05:25:15.302130Z","shell.execute_reply":"2023-12-09T05:25:15.325700Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"new_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\nnew_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.327605Z","iopub.execute_input":"2023-12-09T05:25:15.327881Z","iopub.status.idle":"2023-12-09T05:25:15.337681Z","shell.execute_reply.started":"2023-12-09T05:25:15.327857Z","shell.execute_reply":"2023-12-09T05:25:15.336776Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train_feats = pd.DataFrame()\ntest_feats = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.338709Z","iopub.execute_input":"2023-12-09T05:25:15.338979Z","iopub.status.idle":"2023-12-09T05:25:15.349006Z","shell.execute_reply.started":"2023-12-09T05:25:15.338955Z","shell.execute_reply":"2023-12-09T05:25:15.348211Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"train_feats = new_train_feats.merge(other_train_feats,on='id')\ntest_feats = new_test_feats.merge(other_test_feats,on='id')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.350775Z","iopub.execute_input":"2023-12-09T05:25:15.351153Z","iopub.status.idle":"2023-12-09T05:25:15.376372Z","shell.execute_reply.started":"2023-12-09T05:25:15.351089Z","shell.execute_reply":"2023-12-09T05:25:15.375603Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"data = []\n\nfor logs in [traindf, testdf]:\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n\n    group = logs.groupby('id')['time_diff']\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n\n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:15.377404Z","iopub.execute_input":"2023-12-09T05:25:15.377647Z","iopub.status.idle":"2023-12-09T05:25:23.571981Z","shell.execute_reply.started":"2023-12-09T05:25:15.377625Z","shell.execute_reply":"2023-12-09T05:25:23.570910Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_feats['score_class'] = le.fit_transform(train_feats['score'])","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.573293Z","iopub.execute_input":"2023-12-09T05:25:23.573617Z","iopub.status.idle":"2023-12-09T05:25:23.579960Z","shell.execute_reply.started":"2023-12-09T05:25:23.573590Z","shell.execute_reply":"2023-12-09T05:25:23.578983Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"target_col = ['score']\n\ndrop_cols = ['id', 'score_class']\ntrain_cols = list()\n\ntrain_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n\ntrain_cols.__len__(), target_col.__len__()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.581216Z","iopub.execute_input":"2023-12-09T05:25:23.581587Z","iopub.status.idle":"2023-12-09T05:25:23.595125Z","shell.execute_reply.started":"2023-12-09T05:25:23.581554Z","shell.execute_reply":"2023-12-09T05:25:23.594222Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(656, 1)"},"metadata":{}}]},{"cell_type":"code","source":"nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\nnan_cols","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.596635Z","iopub.execute_input":"2023-12-09T05:25:23.597028Z","iopub.status.idle":"2023-12-09T05:25:23.612224Z","shell.execute_reply.started":"2023-12-09T05:25:23.596995Z","shell.execute_reply":"2023-12-09T05:25:23.611402Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['paragraph_len_std',\n 'paragraph_len_sem',\n 'paragraph_len_skew',\n 'paragraph_word_count_std',\n 'paragraph_word_count_sem',\n 'paragraph_word_count_skew',\n 'sent_len_skew',\n 'sent_word_count_skew']"},"metadata":{}}]},{"cell_type":"code","source":"for col in nan_cols:\n    mode_value_train = train_feats[col].mode()[0]  # In case there are multiple modes, choose the first one\n    train_feats[col].fillna(mode_value_train, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.613379Z","iopub.execute_input":"2023-12-09T05:25:23.613704Z","iopub.status.idle":"2023-12-09T05:25:23.624962Z","shell.execute_reply.started":"2023-12-09T05:25:23.613672Z","shell.execute_reply":"2023-12-09T05:25:23.624222Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"for col in test_feats.columns[test_feats.isna().any()].tolist():\n    # Find the most frequent value in the training set for the current feature\n    most_frequent_value_train = train_feats[col].mode()[0]\n    \n    # Fill missing values in the test set with the most frequent value from the training set\n    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n\ntrain_feats.shape, test_feats.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.626062Z","iopub.execute_input":"2023-12-09T05:25:23.626434Z","iopub.status.idle":"2023-12-09T05:25:23.734529Z","shell.execute_reply.started":"2023-12-09T05:25:23.626401Z","shell.execute_reply":"2023-12-09T05:25:23.733624Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"((2471, 659), (3, 657))"},"metadata":{}}]},{"cell_type":"code","source":"train_feats.columns[train_feats.isna().any()].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.735542Z","iopub.execute_input":"2023-12-09T05:25:23.735802Z","iopub.status.idle":"2023-12-09T05:25:23.744042Z","shell.execute_reply.started":"2023-12-09T05:25:23.735778Z","shell.execute_reply":"2023-12-09T05:25:23.743098Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"nan_values_test = test_feats.columns[test_feats.isna().any()].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.745217Z","iopub.execute_input":"2023-12-09T05:25:23.745580Z","iopub.status.idle":"2023-12-09T05:25:23.752185Z","shell.execute_reply.started":"2023-12-09T05:25:23.745543Z","shell.execute_reply":"2023-12-09T05:25:23.751226Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"clean_memory()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.753329Z","iopub.execute_input":"2023-12-09T05:25:23.753668Z","iopub.status.idle":"2023-12-09T05:25:23.971988Z","shell.execute_reply.started":"2023-12-09T05:25:23.753641Z","shell.execute_reply":"2023-12-09T05:25:23.971193Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"models_dict = {}\nscores = []\n\ntest_predict_list = []\nbest_params = {'boosting_type': 'gbdt', \n               'metric': 'rmse',\n               'reg_alpha': 0.003188447814669599, \n               'reg_lambda': 0.0010228604507564066, \n               'colsample_bytree': 0.5420247656839267, \n               'subsample': 0.9778252382803456, \n               'feature_fraction': 0.8,\n               'bagging_freq': 1,\n               'bagging_fraction': 0.75,\n               'learning_rate': 0.01716485155812008, \n               'num_leaves': 19, \n               'min_child_samples': 46,\n               'verbosity': -1,\n               'random_state': 42,\n               'n_estimators': 500,\n               'device_type': 'cpu'}\n\nfor i in range(5): \n    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n\n    oof_valid_preds = np.zeros(train_feats.shape[0], )\n\n    X_test = test_feats[train_cols]\n\n\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n\n        print(\"==-\"* 50)\n        print(\"Fold : \", fold)\n\n        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n\n        print(\"Trian :\", X_train.shape, y_train.shape)\n        print(\"Valid :\", X_valid.shape, y_valid.shape)\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            'random_state': 42,\n            \"n_estimators\" : 12001,\n            \"verbosity\": -1,\n            \"device_type\": \"cpu\",\n            **best_params\n        }\n\n        model = lgb.LGBMRegressor(**params)\n\n        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n        verbose_callback = lgb.callback.record_evaluation({})\n\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n                  callbacks=[early_stopping_callback, verbose_callback],\n        )\n\n        valid_predict = model.predict(X_valid)\n        oof_valid_preds[valid_idx] = valid_predict\n\n        test_predict = model.predict(X_test)\n        test_predict_list.append(test_predict)\n\n        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n        print(\"Fold RMSE Score : \", score)\n\n        models_dict[f'{fold}_{i}'] = model\n\n\n    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n    scores.append(oof_score)\n    print(\"OOF RMSE Score : \", oof_score)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:25:23.973566Z","iopub.execute_input":"2023-12-09T05:25:23.973959Z","iopub.status.idle":"2023-12-09T05:31:05.739332Z","shell.execute_reply.started":"2023-12-09T05:25:23.973924Z","shell.execute_reply":"2023-12-09T05:31:05.738371Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5757524553175648\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5124801352373705\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6765217864044779\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6210856784372102\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5915426335932219\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6153369985039643\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6395875078349301\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6410026177927232\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6324154235898533\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5598078195825019\nOOF RMSE Score :  0.6082148657675444\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5925296006613595\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5819568328887835\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5681212558294391\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5901839094990252\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.610060978544169\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6380771121784595\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.634496742553001\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5728801316463459\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6565324960729847\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6281223044800924\nOOF RMSE Score :  0.6079832143202831\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6764617435407694\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5923338854986715\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5709743055596099\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.585341443706386\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5751739827978267\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6059104348737351\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6787403675659698\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.562340112531844\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6476001630206506\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5997691547437447\nOOF RMSE Score :  0.6108530583979506\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6158977259580053\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5912418587612724\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6175867419029114\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6038720388658225\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6458393054333451\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5578056133046583\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6338790375237283\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6130463644548536\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6746506613333143\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5502675533424943\nOOF RMSE Score :  0.6114593777884657\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  0\nTrian : (2223, 656) (2223, 1)\nValid : (248, 656) (248, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6003383087384208\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  1\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5752770743102754\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  2\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6200794264692081\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  3\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.603256859863034\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  4\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.5788837452132308\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  5\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6511943267807748\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  6\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6022043523642526\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  7\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6181255157528367\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  8\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.638779651703035\n==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-==-\nFold :  9\nTrian : (2224, 656) (2224, 1)\nValid : (247, 656) (247, 1)\n[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nFold RMSE Score :  0.6136274730470953\nOOF RMSE Score :  0.6105886237680775\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_importances_values = np.asarray([model.feature_importances_ for model in models_dict.values()]).mean(axis=0)\nfeature_importance_df = pd.DataFrame({'name': train_cols, 'importance': feature_importances_values})\n\nfeature_importance_df = feature_importance_df.sort_values('importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.740661Z","iopub.execute_input":"2023-12-09T05:31:05.740985Z","iopub.status.idle":"2023-12-09T05:31:05.755412Z","shell.execute_reply.started":"2023-12-09T05:31:05.740959Z","shell.execute_reply":"2023-12-09T05:31:05.754499Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.756620Z","iopub.execute_input":"2023-12-09T05:31:05.757104Z","iopub.status.idle":"2023-12-09T05:31:05.763158Z","shell.execute_reply.started":"2023-12-09T05:31:05.757078Z","shell.execute_reply":"2023-12-09T05:31:05.762276Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"0.6098198280084642"},"metadata":{}}]},{"cell_type":"code","source":"test_feats['score'] = np.mean(test_predict_list, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.764255Z","iopub.execute_input":"2023-12-09T05:31:05.764563Z","iopub.status.idle":"2023-12-09T05:31:05.771936Z","shell.execute_reply.started":"2023-12-09T05:31:05.764538Z","shell.execute_reply":"2023-12-09T05:31:05.771077Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"publiclgbm_pred = test_feats[['id', 'score']]","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.773255Z","iopub.execute_input":"2023-12-09T05:31:05.773625Z","iopub.status.idle":"2023-12-09T05:31:05.781278Z","shell.execute_reply.started":"2023-12-09T05:31:05.773593Z","shell.execute_reply":"2023-12-09T05:31:05.780508Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"publiclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.782300Z","iopub.execute_input":"2023-12-09T05:31:05.782640Z","iopub.status.idle":"2023-12-09T05:31:05.797009Z","shell.execute_reply.started":"2023-12-09T05:31:05.782616Z","shell.execute_reply":"2023-12-09T05:31:05.796062Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.493897\n1  2222bbbb  1.470117\n2  4444cccc  1.470483","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.493897</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.470117</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.470483</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# lgbm + xgb ensemble go ","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.798019Z","iopub.execute_input":"2023-12-09T05:31:05.798287Z","iopub.status.idle":"2023-12-09T05:31:05.802209Z","shell.execute_reply.started":"2023-12-09T05:31:05.798262Z","shell.execute_reply":"2023-12-09T05:31:05.801260Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nimport re\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import skew, kurtosis\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom scipy.stats import skew, kurtosis\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom functools import partial\nfrom sklearn import model_selection,metrics\nimport optuna\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.803203Z","iopub.execute_input":"2023-12-09T05:31:05.803502Z","iopub.status.idle":"2023-12-09T05:31:05.902719Z","shell.execute_reply.started":"2023-12-09T05:31:05.803466Z","shell.execute_reply":"2023-12-09T05:31:05.901783Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\nactivities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\nevents = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\ntext_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n\n\ndef count_by_values(df, colname, values):\n    fts = df.select(pl.col('id').unique(maintain_order=True))\n    for i, value in enumerate(values):\n        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n        fts  = fts.join(tmp_df, on='id', how='left') \n    return fts\n\n\ndef dev_feats(df):\n    \n    print(\"< Count by values features >\")\n    \n    feats = count_by_values(df, 'activity', activities)\n    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n\n    print(\"< Input words stats features >\")\n\n    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n    temp = temp.drop('text_change')\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Numerical columns features >\")\n\n    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< Categorical columns features >\")\n    \n    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Idle time features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n                                   inter_key_median_lantency = pl.median('time_diff'),\n                                   mean_pause_time = pl.mean('time_diff'),\n                                   std_pause_time = pl.std('time_diff'),\n                                   total_pause_time = pl.sum('time_diff'),\n                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n    feats = feats.join(temp, on='id', how='left') \n    \n    print(\"< P-bursts features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('time_diff')<2)\n    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< R-bursts features >\")\n\n    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n    feats = feats.join(temp, on='id', how='left')\n    \n    return feats\n\n\ndef train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x.iloc[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x.iloc[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\ndef evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n        model.fit(train_x, train_y)\n        if test_x is None:\n            test_y[valid_index] = model.predict(valid_x)\n        else:\n            test_y[:, i] = model.predict(test_x)\n    return test_y if (test_x is None) else np.mean(test_y, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.904164Z","iopub.execute_input":"2023-12-09T05:31:05.904479Z","iopub.status.idle":"2023-12-09T05:31:05.942627Z","shell.execute_reply.started":"2023-12-09T05:31:05.904451Z","shell.execute_reply":"2023-12-09T05:31:05.941820Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.25)\ndef q3(x):\n    return x.quantile(0.75)\n\nAGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n\ndef reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\n\ndef get_essay_df(df):\n    df       = df[df.activity != 'Nonproduction']\n    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n    return essay_df\n\n\ndef word_feats(df):\n    essay_df = df\n    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('word')\n    df['word_len'] = df['word'].apply(lambda x: len(x))\n    df = df[df['word_len'] != 0]\n\n    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n    word_agg_df['id'] = word_agg_df.index\n    word_agg_df = word_agg_df.reset_index(drop=True)\n    return word_agg_df\n\n\ndef sent_feats(df):\n    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('sent')\n    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n    df = df[df.sent_len!=0].reset_index(drop=True)\n\n    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n\ndef parag_feats(df):\n    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n    df = df.explode('paragraph')\n    # Number of characters in paragraphs\n    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n    df = df[df.paragraph_len!=0].reset_index(drop=True)\n    \n    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n\ndef product_to_keys(logs, essays):\n    essays['product_len'] = essays.essay.str.len()\n    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n    essays = essays.merge(tmp_df, on='id', how='left')\n    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n    return essays[['id', 'product_to_keys']]\n\ndef get_keys_pressed_per_second(logs):\n    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n    return temp_df[['id', 'keys_per_second']]","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.943800Z","iopub.execute_input":"2023-12-09T05:31:05.944107Z","iopub.status.idle":"2023-12-09T05:31:05.975137Z","shell.execute_reply.started":"2023-12-09T05:31:05.944080Z","shell.execute_reply":"2023-12-09T05:31:05.974232Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\ntrain_logs    = pl.scan_csv(data_path + 'train_logs.csv')\ntrain_feats   = dev_feats(train_logs)\ntrain_feats   = train_feats.collect().to_pandas()\n\nprint('< Essay Reconstruction >')\ntrain_logs             = train_logs.collect().to_pandas()\ntrain_essays           = get_essay_df(train_logs)\ntrain_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\ntrain_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\ntrain_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n\n\nprint('< Mapping >')\ntrain_scores   = pd.read_csv(data_path + 'train_scores.csv')\ndata           = train_feats.merge(train_scores, on='id', how='left')\nx              = data.drop(['id', 'score'], axis=1)\ny              = data['score'].values\nprint(f'Number of features: {len(x.columns)}')\n\n\nprint('< Testing Data >')\ntest_logs   = pl.scan_csv(data_path + 'test_logs.csv')\ntest_feats  = dev_feats(test_logs)\ntest_feats  = test_feats.collect().to_pandas()\n\ntest_logs             = test_logs.collect().to_pandas()\ntest_essays           = get_essay_df(test_logs)\ntest_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\ntest_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\ntest_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n\n\ntest_ids = test_feats['id'].values\ntestin_x = test_feats.drop(['id'], axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:31:05.976595Z","iopub.execute_input":"2023-12-09T05:31:05.976931Z","iopub.status.idle":"2023-12-09T05:32:25.440731Z","shell.execute_reply.started":"2023-12-09T05:31:05.976905Z","shell.execute_reply":"2023-12-09T05:32:25.439775Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n< Essay Reconstruction >\n< Mapping >\nNumber of features: 165\n< Testing Data >\n< Count by values features >\n< Input words stats features >\n< Numerical columns features >\n< Categorical columns features >\n< Idle time features >\n< P-bursts features >\n< R-bursts features >\n","output_type":"stream"}]},{"cell_type":"code","source":"print('< LGBM Learning and Evaluation >')\nlgbm_param = {'n_estimators': 1024,\n         'learning_rate': 0.005,\n         'metric': 'rmse',\n         'random_state': 42,\n         'force_col_wise': True,\n         'verbosity': 0,}\nlgbm_solution = LGBMRegressor(**lgbm_param)\nlgbm_pred   = evaluate(x.copy(), y.copy(), lgbm_solution, test_x=testin_x.copy()) \n\nlgbm_sub = pd.DataFrame({'id': test_ids, 'score': lgbm_pred})\n#sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:32:25.442055Z","iopub.execute_input":"2023-12-09T05:32:25.442816Z","iopub.status.idle":"2023-12-09T05:32:59.294877Z","shell.execute_reply.started":"2023-12-09T05:32:25.442779Z","shell.execute_reply":"2023-12-09T05:32:59.293933Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"< LGBM Learning and Evaluation >\n","output_type":"stream"}]},{"cell_type":"code","source":"lgbm_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:32:59.296334Z","iopub.execute_input":"2023-12-09T05:32:59.297115Z","iopub.status.idle":"2023-12-09T05:32:59.308277Z","shell.execute_reply.started":"2023-12-09T05:32:59.297075Z","shell.execute_reply":"2023-12-09T05:32:59.307220Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.263671\n1  2222bbbb  1.258523\n2  4444cccc  1.260993","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.263671</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.258523</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.260993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('< XGB Learning and Evaluation >')\n\nxgb_param={\n'reg_alpha': 0.0008774661176012108,\n'reg_lambda': 2.542812743920178,\n'colsample_bynode': 0.7839026197349153,\n'subsample': 0.8994226268096415,\n'eta': 0.04730766698056879, \n'max_depth': 3, \n'n_estimators': 1024,\n'random_state': 42,\n'eval_metric': 'rmse'\n}\n\nxgb_solution = XGBRegressor(**xgb_param)\nxgb_pred   = evaluate(x.copy(), y.copy(), xgb_solution, test_x=testin_x.copy()) \n\nxgb_sub = pd.DataFrame({'id': test_ids, 'score': xgb_pred})\n#xgb_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:32:59.309898Z","iopub.execute_input":"2023-12-09T05:32:59.310321Z","iopub.status.idle":"2023-12-09T05:33:18.044452Z","shell.execute_reply.started":"2023-12-09T05:32:59.310282Z","shell.execute_reply":"2023-12-09T05:33:18.043417Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"< XGB Learning and Evaluation >\n","output_type":"stream"}]},{"cell_type":"code","source":"xgb_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.045782Z","iopub.execute_input":"2023-12-09T05:33:18.046176Z","iopub.status.idle":"2023-12-09T05:33:18.055379Z","shell.execute_reply.started":"2023-12-09T05:33:18.046147Z","shell.execute_reply":"2023-12-09T05:33:18.054340Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  2.674548\n1  2222bbbb  1.385251\n2  4444cccc  1.398941","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2.674548</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.385251</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.398941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"best_w =0.81 # 밑 주석 처리된 부분에서 최적의 best_w  blending으로 도출 \n\nW = [best_w, 1 - best_w]\nprint(W)\nensemble_preds = lgbm_pred * W[0] + xgb_pred * W[1]\nensemble_preds","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.056688Z","iopub.execute_input":"2023-12-09T05:33:18.057037Z","iopub.status.idle":"2023-12-09T05:33:18.075016Z","shell.execute_reply.started":"2023-12-09T05:33:18.056999Z","shell.execute_reply":"2023-12-09T05:33:18.074131Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"[0.81, 0.18999999999999995]\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"array([1.5317379 , 1.28260158, 1.28720322])"},"metadata":{}}]},{"cell_type":"code","source":"ensemble_sub = pd.DataFrame({'id': test_ids, 'score': ensemble_preds})\nensemble_sub","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.076112Z","iopub.execute_input":"2023-12-09T05:33:18.076395Z","iopub.status.idle":"2023-12-09T05:33:18.089287Z","shell.execute_reply.started":"2023-12-09T05:33:18.076371Z","shell.execute_reply":"2023-12-09T05:33:18.088410Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.531738\n1  2222bbbb  1.282602\n2  4444cccc  1.287203","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.531738</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.282602</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.287203</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"publiclgbm_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.090341Z","iopub.execute_input":"2023-12-09T05:33:18.090679Z","iopub.status.idle":"2023-12-09T05:33:18.103318Z","shell.execute_reply.started":"2023-12-09T05:33:18.090652Z","shell.execute_reply":"2023-12-09T05:33:18.102397Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.493897\n1  2222bbbb  1.470117\n2  4444cccc  1.470483","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.493897</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.470117</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.470483</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 두 데이터프레임을 'id' 컬럼을 기준으로 병합\nmerged_df = ensemble_sub.merge(publiclgbm_pred, on='id', suffixes=('_ensemble', '_publiclgbm'))\n\n# 가중치를 반반으로 설정\nweight_ensemble = 0.5\nweight_publiclgbm = 0.5\n\n# 가중 평균 점수 계산하여 원래의 'score' 컬럼에 저장\nmerged_df['score'] = (merged_df['score_ensemble'] * weight_ensemble) + (merged_df['score_publiclgbm'] * weight_publiclgbm)\nmerged_df.drop(['score_ensemble', 'score_publiclgbm'], axis=1, inplace=True)\n# 결과 출력\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.104623Z","iopub.execute_input":"2023-12-09T05:33:18.104968Z","iopub.status.idle":"2023-12-09T05:33:18.119607Z","shell.execute_reply.started":"2023-12-09T05:33:18.104935Z","shell.execute_reply":"2023-12-09T05:33:18.118655Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"         id     score\n0  0000aaaa  1.512818\n1  2222bbbb  1.376359\n2  4444cccc  1.378843","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1.512818</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>1.376359</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>1.378843</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:33:18.120736Z","iopub.execute_input":"2023-12-09T05:33:18.121071Z","iopub.status.idle":"2023-12-09T05:33:18.129127Z","shell.execute_reply.started":"2023-12-09T05:33:18.121044Z","shell.execute_reply":"2023-12-09T05:33:18.128305Z"},"trusted":true},"execution_count":68,"outputs":[]}]}